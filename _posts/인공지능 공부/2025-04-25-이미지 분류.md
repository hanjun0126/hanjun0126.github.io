---
categories: [CV]
description: 이미지 분류 내용 정리
tags: [CV]
math: true
---

# Image Classification

**Image classification** refers to algorithms that recognize and categorize elemenets such as objects or scenes within an image. It is a type of supervised learning, where a dataset is constructed by assigning labels to images according to their corresponding classes.

It is one of the most commonly used algorithms in the filed of computer vision and is mainly used to classify visual elemets. In other words, it involves determining which of the predefined classes is most similar to the image input by user. Image classification is generally divided into single-class classification, multi-class classification, and multi-label classification.

**Single-class classification** categorizes an image into only one representative class, even if there are multiple different objects in the image. For example, if an image of a dog is given, it is classified as either a dog or not a dog. In other words, it determines whether the class corresponging to the image is true or false.

**Multi-class classification** performs a relatively more complex task than single-class classification. While single-class classification determins true or false, multi-class classification distinguishes between categories such as dog or cat, or it may classify the breed of a dog.

**Multi-label classification** predicts multiple classes from a input image. Unlike multi-class classification, multi-label classification can identify several classes within one image. For example, in multi-label classification, the model might detect a dog, a sofa, and blinds with in the same image. In general, because the classifier must handle multiple labels simultaneously, multi-label classification requires more data and a more complex model structure than single-class or multi-class classification.

There are various approaches to algorithms that perform image classification, ranging from rule-based algorithms to machine learning methods. Some of the techniques used in image classification are as follows.

>   1.   Support Vector Machine(SVM)
>   2.   K-nearest Neighbors Algorithm(KNN)
>   3.   Decision Tree
>   4.   Artificial Neural Network(ANN)
>   5.   Convolutional Neural Network(CNN)

There are several ways to classify images, but here I will focus on models built using convolutional neural networks.

## AlexNet

AlexNet is a convolutional neural network model that won the ILSVRC(ImageNet Large Scale Visual Recognition Challenge) competition in 2012. Before 2012, models typically had shallow architectures and  an error rate of around 26%, but AlexNet significantly reduced the error rate to about 16%.

The emergence of AlexNet led to a revival of convolutional neural network models, and from that point on, deeper architectures began to appear and dominate competitions.

Although AlexNet differs in detail from modern convolutional models, its overall structure is similar. Let's explore how convolutional neural networks achieve high performance through the example of AlexNet.

AlexNet uses convolutional and max-pooling layers to extract features from images, and then classifies them through fully connected layers. Looking at AlexNet's structure, we can see that during the forward propagation, the number of channels in the feture maps increases while their spatial size decrease. In covolutional models, increasing the number of feature map channels enhances the model's representational power, while reducing their size helps lower computational cost. Like AlexNet, most convolutional networks are designed to reduce the spatial size of feature maps while increasing the number of channels.

### LeNet-5 and AlexNet

The structure of AlexNet is similar to that of LeNet-5, which was developed in 1998. Let's first take a look at the architecture of LeNet-5. LeNet-5 consisits of one input layer(I), two convolutional layers(C1, C3), two subsampling layers(S2, S4), one fully-connected layer(FC5), and one output layer(O). For subsampling, average-pooling is used, and the actication functions include sigmoid at the output layer and hyperbolic tangent(tanh) in the other layers.

The layers of AlexNet consist of one input layer(I), five convolutional layers(C1, C3, C5, C6, C7), three fully-connected layers(FC9, FC10, FC11), and one output layer(O). Max-pooling is used for subsampling, and the ReLU activation fuction is applied.

The layers of AlexNet consist of one input layer, five convolutional layers, three subsampling layers, three fully connected layers, and one output layer. Max pooling is used for subsampling, and the ReLU activation function is applied.

The key differences between AlexNet and LeNet-5 are the input image size, the activation function, the pooling method, and the addition of dropout. LeNet-5 used a rescaled logistic sigmoid function as its activation function, whereas AlexNet adopted ReLU.

While the sigmoid function can lead to the vanishing gradient problem, ReLu is a non-linear activation function that does not cause this issue. The vanishing gradient problem hinders the stacking of deep layers, so by using ReLU, deeper architectures became possible.

While the sigmoid function can lead to the vanishing gradient problem, ReLU is a non-linear activation function that does not cause this issue. The vanishing gradient problem hinders the stacking of deep layers, so by using ReLU, deeper architectures became possible.

There was also a change in the pooling method: AlexNet used max-pooling to consolidate and simplify values. When comparing average-pooling and max-pooling, max-pooling tends to produce more consistent value distributions, which in turn makes gradient calculation easier.

Lastly, AlexNet incorporated dropout, which wa snecessary due to its significantly deeper architecture compared to LeNet-5. As the number of model parameters increases, the risk of overfitting also rises, so dropout was used to mitigate this problem.

Fully-connected layers, which receive global features as input, typically require far more parameters than convolutional layers. By using dropout to address this overfitting problem, AlexNet was able to perform twice as many training iterations, leading to improved performance.

#### Model Training

The torchinfo library provides functionality to check the layers used in a model, as well as the input and output shapes and the total number of parameters. By using the model summary function, you can see the output shape and number of parameters for each layer when a specific **model** and **input data** are passed. The output shapes change depending on the shape of the tensor data fed into the model.

The following example shows how to load AlexNet and display its model architecture.

```python
from torchvision import models
from torchinfo import summary
from torchvision.models import AlexNet_Weights

model = models.alexnet(weights="AlexNet_Weights.IMAGENET1K_V1")
summary(model, (1, 3, 224, 224), device="cpu")
```

```python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AlexNet                                  [1, 1000]                 --
├─Sequential: 1-1                        [1, 256, 6, 6]            --
│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296
│    └─ReLU: 2-2                         [1, 64, 55, 55]           --
│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --
│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392
│    └─ReLU: 2-5                         [1, 192, 27, 27]          --
│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --
│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936
│    └─ReLU: 2-8                         [1, 384, 13, 13]          --
│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992
│    └─ReLU: 2-10                        [1, 256, 13, 13]          --
│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080
│    └─ReLU: 2-12                        [1, 256, 13, 13]          --
│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --
├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --
├─Sequential: 1-3                        [1, 1000]                 --
│    └─Dropout: 2-14                     [1, 9216]                 --
│    └─Linear: 2-15                      [1, 4096]                 37,752,832
│    └─ReLU: 2-16                        [1, 4096]                 --
│    └─Dropout: 2-17                     [1, 4096]                 --
│    └─Linear: 2-18                      [1, 4096]                 16,781,312
│    └─ReLU: 2-19                        [1, 4096]                 --
│    └─Linear: 2-20                      [1, 1000]                 4,097,000
==========================================================================================
Total params: 61,100,840
Trainable params: 61,100,840
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 714.68
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 3.95
Params size (MB): 244.40
Estimated Total Size (MB): 248.96
==========================================================================================
```

The output shows the size of the feature maps produced by each layer and the number of parameters required when a tensor of size (1, 3, 224, 224) is input into AlexNet. "Layer", "Output Shape", "Param" refer to the type and structure of each layer, the size of the output tensor, and the number of parameters used in that layer, respectively.

Using the torchvision library, we loaded a pretrained AlexNet Model that was trained on ImageNet. Since ImageNet is a dataset composed of 1,000 classes, the pretrained AlexNet performs predictions for those 1,000 classes.

The following example allows you to check the class labels used by AlexNet. And we can check the contents of this file(
"imagenet_classes.txt") at this [page][https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt]. 

```python
with open("../datasets/imagenet_classes.txt", "r") as file:
    classes = file.read().splitlines()

print(f"number of classes : {len(classes)}")
print(f"first class label : {classes[0]}")
```

```python
Number of classes : 1000
First class label : tench
```

AlexNet uses input images of size 256 by 256 and applies normalization using the mean and standard deviation of the RGB pixel values. The following example demonstrates how to preprocess an image in the same way as the data AlexNet was trained on, using a composed transform pipeline.

```python
import torch
from PIL import Image
from torchvision import models, transforms

transform = transforms.Compose(
    [
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
    ]
)

device = "mps" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else "cpu"
model.eval().to(device)

tensors = []
files = ["../datasets/images/airplane.jpg", "../datasets/images/bus.jpg"]
for file in files:
    image = Image.open(file)
    tensors.append(transform(image))

tensors = torch.stack(tensors)
print(f"Input tensor size : {tensors.shape}")
```

```python
Input tensor size : torch.Size([2, 3, 224, 224])
```

Above preprocessing is performed in the same way as the image data used to train AlexNet. During this process, the normalization class uses specific mean and standard deviation values, referred to [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225], respectively. These values represent the mean and standard deviation of the images used in the ImageNet dataset.

ImageNet is a large-scale visual recognition dataset widely used in the filed of computer vision. With over 14 million images, it provides optimal normalization values for images related to natural or everyday scenes.

However, if the image dataset is composed of domain-specific data- such as satellite or medical images, it is recommended to compute and apply the mean and standard deviation that are appropriate for that particular dataset.

#### Model Inference

The example below performs inference using the model and input data, then prints the top five predicted classes along with their probailites.

```python
import numpy as np
from torch.nn import functional as F

with torch.no_grad():
    outputs = model(tensors.to(device))
    probs = F.softmax(outputs, dim=-1)
    top_probs, top_idxs = probs.topk(5)

top_probs = top_probs.detach().cpu().numpy()
top_idxs = top_idxs.detach().cpu().numpy()
top_classes = np.array(classes)[top_idxs]

for idx, (cls, prob) in enumerate(zip(top_classes, top_probs)):
    print(f"{files[idx]} Inference Result")
    for c, p in zip(cls, prob):
        print(f" - {c:<30} : {p * 100:>5.2f}%")
```

```python
../datasets/images/airplane.jpg Inference Result
 - airliner                       : 66.83%
 - warplane                       : 20.12%
 - wing                           :  9.29%
 - space shuttle                  :  2.89%
 - missile                        :  0.38%
../datasets/images/bus.jpg Inference Result
 - streetcar                      : 60.25%
 - trolleybus                     : 37.99%
 - minibus                        :  1.54%
 - passenger car                  :  0.17%
 - recreational vehicle           :  0.03%
```

Since the model is being evaluated with arbitrary input, gradient computation is disabled using `torch.no_grad`, and a forward propagation is performed by passing the input tensor through the model. The returned output has the same batch size as the input and follows the shape [batch size, number of classes].

The tensor is passed through a softmax function to convert the outputs into probabilities, and the `topk` method is used to retrieve the top five values with the highest probabilities. `top_probs` returns the five highest probability valuese, and `top_idxs` indicates the corresponding indices of those values. To make the output more human-readable, the probabilities, indices, and class labels are converted into NumPy arrays.

If we print the transformed, human-readable class names along with their predicted probabilities, we can observe that for the "airplane.jpg" file, classes such as *airliner* and *warplane*, which are similar to airplanes, are predicted with the highest probabilities. Likewise, for the "bus.jpg" file, classes such as *streetcar* and *trolleybus*, which are related to buses, are predicted as the most likely.

## VGG

**VGG-16** is a convolutional neural network model that placed second in the 2014 ILSVRC competition. It was developed by the VGG(Visual Geometry Group) research team at the University of Oxford. Although **GoogleNet**, the winning model of the same competition, achieved a lower error rate of about 6% compared to VGG-16's 7%, VGG-16 has been more widely used in subsequent research.

GoogleNet uses an **Inception Module** to perform parallel convolution operations with various filters sizes and pooling operations. This approach captures both global and local features, improving performance. However, due to its complex structure, it hasn't been as widely adopted as the relatively simpler VGG-16 model.

The VGG-16 model consists of 16 layers - 13 convolutional layers and 3 fully-connected layers. A key feature of the VGG architecture is its use of only small 3 by 3 filters for convolution, allowing for a deeper network structure.

VGG-16 has demonstrated high performance across various image recognition tasks and has served as the foundation for many subsequent deep learning models.

### AlexNet and VGG-16

AlexNet and VGG-16 are both convoltional models used for image recognition and share many similarities. The similarities between AlexNet and VGG-16 mainy refer to the training data and the overall model architecture.

Both AlexNet and VGG-16 were trained on the ImageNet dataset. AlexNet was trained using approximately 1.2 million images, whereas VGG-16 was trained on around 14 million images.

They also have similar model structures - both use convolutional and fully-connected layers in a sequential design - but VGG-16 is composed of roughly twice as many layers as AlexNet, making it significantly deeper while maintaining a similar architectural style.



VGG-16, like AlexNet, is designed using convolutional layers, ReLU activations, pooling layers, and fully-connected layers. However, it differs in the size of convolutional filters and the number of layers used.

In AlexNet, the first convolutional layer uses 11 by 11 filters, and the second uses 5 by 5 filters. These relatively large filters allow AlexNet to capture wide **receptive fields** early in the network. In contrast, VGG-16 opts for multiple 3 by 3 filters, aiming to analyze image features more precisely.

In convolutional neural networks, a larger receptive field allows a node to observe a boarder area of the image, which helps in learning **global features** more efficiently. However, it can be less effective at capturing low-level **local features** like edges and corners.

VGG-16 addresses this by applying small 3 by 3 filters repeatedly, effectively simulating larger receptive fields like 7 by 7, but with added benefits. Using smaller filters multiple times not only reduces the number of parameters but also increases non-linearity through more frequent activation functions. For instance, a single 7 by 7 filters requires 49 parameters, whereas three 3 by 3 filters require only 27 parameters in total.

Using multiple small filters allows for greater non-linearity. Since convolutional layers themselves are linear, activation functions are always added after them. Therefore, the more convolutional layers a model has, the more non-linear operations are introduced, enhancing the model's representational power.

In the case of VGG-16, the number of channels in the convolutional layers increases up to 512, whereas in AlexNet, it only goes up to 384. The number of channels in a feature map can be seen as the amount of space available for storing information. A higher number of channels means more information can be stored in a feature map of the same size, which improves the model's ability to express complex features.

Thanks to these improvements, VGG-16 achieved an error rate of around 7%, which is approximately a 9% performance improvement over AlexNet. This makes VGG-16 a highly influential convolutional neural network model for image recognition and classification tasks.



We will now load the VGG-16 model. The VGG-16 loading function allows you to load a pretrained weight parameters. Just like with AlexNet, if you do not specify the pretrained weights or set them to None, the model will be loaded without any pretrained parameters.

Compared to AlexNet, the VGG-16 demonstrates higher performance. Since both models were trained on the ImageNet dataset, they can each recognize 1,000 classes. To load pretrained weights for VGG-16, you can pass  VGG16_Weights.IMAGENET1K_V1 as an argument. When printing the model structure, you can see that it is broadly divided into three parts: feature extraction, average pooling, and classification.

If you check the final linear transformation in the classifier layer, it returns an output dimension of 1,000. This value represents the number of categories the model can classify.

#### Model training

```python
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision import models
from torchvision import transforms
from torchvision.datasets import ImageFolder

hyperparams = {
    "batch_size": 4,
    "learning_rate": 0.0001,
    "epochs": 5,
    "transform": transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48235, 0.45882, 0.40784],
                std=[1.0/255.0, 1.0/255.0, 1.0/255.0],
            ),
        ]
    ),
}

train_dataset = ImageFolder("../datasets/pet/train", transform=hyperparams["transform"])
test_dataset = ImageFolder("../datasets/pet/test", transform=hyperparams["transform"])

train_dataloader = DataLoader(train_dataset, batch_size=hyperparams["batch_size"], shuffle=True, drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=hyperparams["batch_size"], shuffle=True, drop_last=True)

model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")
model.classifier[6] = nn.Linear(4096, len(train_dataset.classes))

device = "mps" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else "cpu"

model = model.to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.SGD(model.parameters(), lr=hyperparams["learning_rate"])

for epoch in range(hyperparams["epochs"]):
    cost = 0.0

    for images, classes in train_dataloader:
        images = images.to(device)
        classes = classes.to(device)

        output = model(images)
        loss = criterion(output, classes)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cost += loss

    cost = cost/len(train_dataloader)
    print(f"Epoch : {epoch+1:4d}, Cost : {cost:.3f}")

with torch.no_grad():
    model.eval()

    accuracy = 0.0
    for images, classes in test_dataloader:
        images = images.to(device)
        classes = classes.to(device)

        outputs = model(images)
        probs = F.softmax(outputs, dim=-1)
        outputs_classes = torch.argmax(probs, dim=-1)

        accuracy += int(torch.eq(classes, outputs_classes).sum())
    print(f'acc@1 : {accuracy/(len(test_dataloader)*hyperparams["batch_size"]) * 100:.2f}%')
torch.save(model.state_dict(), "../models/VGG16.pt")
print("Saved the model weights")
```

```python
Epoch :    1, Cost : 0.285
Epoch :    2, Cost : 0.088
Epoch :    3, Cost : 0.057
Epoch :    4, Cost : 0.041
Epoch :    5, Cost : 0.031
acc@1 : 97.33%
Saved the model weights
```

Since VGG-16 is already pretrained on dog and cat data, we can observe that the cost decreases stably even with a small dataset and a small size of epochs. Once training is complete, we can evaluate the model using the test dataset by calculating acc@1.

acc@1 measures the accuracy of the top-1 predicted label. Using the `torch.eq` function, we compare the model's predictions with the true classes in the test dataset. `torch.eq` returns "True" if the predicted and actual values match, and by applying `sum` method, we accumulate the number of correct predictions in the accuracy vaiable.

Since the number of correct predictions is stored in the accuracy variable, dividing it by the total number of samples yields the prediction accuracy on the test dataset. This confirms that the model can learn stably even with a small-scale dataset.

#### ResNet

