---
title: 인공지능기초
author: Hanjun
date: 2025-05-27
category: Jekyll
layout: post
mermaid: true
---

## 손실 함수(Loss Function)

손실 함수는 단일 샘플의 실젯값과 예측값의 차이가 발생했을 때 오차가 얼마인지 계산하는 함수를 의미한다. 인공 신경망은 실젯값과 예측값을 통해 계산된 오찻값을 최소화해 정확도를 높이는 방법으로 학습이 진행된다. 그래서 각 데이터의 오차를 계산하는데, 이때 손실 함수를 사용한다.

$H(x)=Wx+b$ 라는 예측 모델이 있을 때, 모집단에서 $x$ 의 값을 모델의 입력으로 주었을 때, 나오는 결과를 예측값이라 하고, 오차는 (실젯값 - 예측값)이 된다. 오차를 통해 예측값이 얼마나 실젯값을 잘 표현하는지 알 수 있다. 하지만 이는 각각의 데이터에 대한 표현일 뿐, 모델이 얼마나 정확히 예측했느지를 알기 위해서는 모든 데이터의 오차를 활용해야한다. 

### 평균 제곱 오차(Mean Squared Error, MSE)

평균 제곱 오차는 제곱 오차(Squared Error, SE)와 오차 제곱합(Sum of Squared Error, SSE)로부터 비롯되었다.

1.   $SE=(Y_i-\hat{Y_i})^2$ 

     ▸ 제곱 오차를 쓰는 이유는 오차의 방향보다는 크기가 중요하기 때문이다. 또한 절대값이 아닌 제곱을 함으로써 오차의 간극을 빠르게 확인할 수 있다.

2.   $SSE=\sum_{i=1}^n(Y_i-\hat{Y_i})^2$

     ▸ 제곱 오차는 각 데이터의 오차일 뿐 모델 자체가 정확히 예측하는지 알 수 없다. 그러므로 모든 제곱 오차를 더해 하나의 값으로 모델을 평가할 수 있다.

결국 MSE는 SSE에서 평균을 구한 방법이다. 데이터가 많아질수록 SSE는 큰 값을 갖게 되는데, 이는 오차가 큰 것인지, 데이터가 많은 것인지에 대한 구분의 필요성을 준다. 
$$
MSE={1\over n}\sum_{i=1}^n(Y_i-\hat{Y_i})^2
$$
MSE는 모델의 성능을 측정할 수 있으며, 오차가 0에 가까워질수록 높은 성능을 갖게 된다. 주로 회귀 분석에서 많이 사용되는 손실 함수이다. MSE에 제곱근을 취하는 경우 평균 제곱근 오차(Root Mean Squared Error. RMSE)라 부르는데, 이는 제곱근을 통해 MSE에서 발생한 왜곡을 감소시키면 정밀도를 표현하기에 적합한 형태가 되기 때문이다.

### 교차 엔트로피(Cross-Entropy)

MSE는 연속형 변수에 사용되는 손실 함수이고, 이산형 변수에는 교차 엔트로피가 손실 함수로 사용된다. 예를 들어 고양이, 개 등을 분류하는 문제에 사용된다. 교차 엔트로피는 실젯값의 확률분포와 예측값의 확률분포 차이를 계산한다.
$$
CE(y,\hat{y})=-\sum_jy_j\log\hat{y_j}
$$
<img src="../assets/gitbook/images/인공지능기초/cross entropy.png" style="zoom:30%;" />

위의 그림은 고양이, 호랑이, 개를 분류하는 목적으로 교차 엔트로피의 손실값을 계산하는 예시이다. 결과적으로 고양이의 예측확률과 실제확률만 계산되는 것을 확인할 수 있다. 고양이 예측확률이 0.99라면 교차 엔트로피는 0.01로 손실값이 매우 낮고, 0.001이면 6.9로 손실값이 매우 높다.

## 활성화 함수(Activation Function)

활성화 함수란 **인공 신경망에서 사용되는 은닉층을 활성화하기 위한 함수**다. 여기서 활성화란 인공 신경망 뉴런의 출력값을 선형에서 비선형으로 변환하는 것이다. 즉, 활성화 함수는 네트워크가 데이터의 복잡한 패턴을 기반으로 학습하고 결정을 내릴 수 있게 제어한다.

활성화 함수는 가중치와 편향으로 이루어진 노드를 **선형에서 비선형으로 갱신하는 역할**을 한다. 직관적으로 네트워크에 포함된 노드는 출력값에 동일한 여향을 미치지 않는다. 즉, **노드마다 전달돼야 하는 정보량이 다르다.** 연산 과정에서 중요한 정보는 더 많이 활성화돼야 하며, 덜 중요한 정보는 비교적 비활성화돼야 한다.

활성화 함수는 비선형 구조를 가져 역전파 과정에서 미분값을 통해 학습이 진행될 수 있게 한다. 활성화 함수가 선형 구조라면, 미분 과정에서 항상 상수가 나오므로 학습을 진행하기가 어렵다. 다시 말해 활성화 함수는 입력을 정규화하는 과정으로 볼 수 있다.

### 이진분류

이진 분류란 규칙에 따라 입력된 값을 두 그룹(True or False/A or B)으로 분류하는 작업을 의미한다. 참 또는 거짓으로 결과를 분류하기 때문에 논리 회귀(로지스틱 회귀, Logistic Regression) 또는 논리 분류(Logistic Classification)라고도 부른다. 

### 시그모이드 함수(Sigmoid Function)

시그모이드 함수는 S자형 곡선 모양으로 반환 값은 0~1의 범위를 갖는다. $x$ 의 계수에 따라 곡선의 경사를 정할 수 있다. 계수가 0에 가까워질수록 완만한 경사를 갖게 되며, 0에서 멀어질수록 급격한 경사를 갖게 된다.
$$
Sigmoid(x)={1\over1+e^{-x}}
$$
시그모이드 함수는 유연한 미분값을 가지므로, 입력에 따라 값이 급격하게 변하지 않는다는 장점이 있다. 또한, 출력 값의 범위가 0~1 사이로 제한됨으로써 정규화 중 **기울기 폭주(Exploding Gradient)** 문제가 발생하지 않고 미분 식이 단순한 형태를 지닌다.

시그모이드 함수는 기울기 폭주를 방지하는 대신 **기울기 소실(Vanishing Gradient)** 문제를 일으킨다. 신경망은 기울기를 이용해 최적화된 값을 찾아가는데, 계층이 많아지면 점점 값이 0에 수렴되는 문제가 발생해 성능이 떨어진다. 그 외에도 Y 값의 중심이 0이 아니므로 입력 데이터가 항상 양수인 경우라면, 기울기는 모두 양수 또는 음수가 되어 기울기가 지그재그 형태로 변동하는 문제점이 발생해 학습 효율성을 감소시킬 수 있다.

### 이진 교차 엔트로피(Binary Cross Entropy, BCE)

MSE는 이진 분류에 좋지 않다. 이는 예측값과 실젯값의 차이가 작으면 계산되는 오차 또한 크기가 작아져 학습을 원할하게 진행하기 어렵기 때문이다. 이러한 경우, 이진 교차 엔트로피가 좋은 대책이 된다. 
$$
\mathbf{BCE}=\mathbf{BCE_1}+\mathbf{BCE_2}=-(Y_i\cdot\mathbf{log}(\hat{Y_l})+(1-Y_i)\cdot\mathbf{log}(1-\hat{Y_l}))
$$
이진 교차 엔트로피는 두 가지 로그 함수를 교차해 오차를 계산한다. $\mathbf{BCE_1}$ 수식은 실젯값이 1일 떄 적용되는 수식이며, $\mathbf{BCE_2}$ 수식은 실젯값이 0일 때 적용하는 수식이다.

<img src="../assets/gitbook/images/인공지능기초/BCE.png" style="zoom:30%;" />

기존의 MSE는 불일치하는 경우에도 높은 손실 값을 반환하지 않았다. 하지만 로그 함수는 로그의 진수가 0에 가까워질수록 무한대로 발산하는 특성이 있다. 그러므로 로그 함수의 경우 불일치하는 비중이 높을수록 높은 손실 값을 반환하게 된다.

로그 함수의 경우 한쪽으로는 무한대로 이동하며 다른 한쪽으로는 0에 가까워지기 때문에 기울기가 0이 되는 지점을 찾기 위해 두 가지 로그함수를 하나로 합쳐 사용한다. 두 로그 함수를 하나로 합친다면, 기울기가 0이 되는 지점을 찾을 수 있게 된다. 최종으로 반환되는 BCE는 오차를 계산하기 위해 각 손실 값의 평균을 반환한다.
$$
\mathbf{BCE}=-{1\over n}\sum_{i=1}^n(Y_i\cdot\mathbf{log}(\hat{Y_l})+(1-Y_i)\cdot\mathbf{log}(1-\hat{Y_l}))
$$
입력 데이터의 분포가 가우시안 분포의 형태를 따른다면 MSE를 사용하고, 베르누이 분포의 형태를 따른다면 CE를 사용한다.

### 계단 함수(Step Function)

계단 함수는 이진 활성화 함수라고도 하며, 퍼셉트론에서 최초로 사용한 활성화 함수다. 입력값의 합이 임계값을 넘으면 0을 출력하고 그렇지 않으면 1을 출력한다.
$$
Step(x)=\left\{ \begin{array}{rcl}{1} & \mbox{if}& 0\leq x \\ 0 & \mbox{else} & otherwise\end{array}\right.
$$
딥러닝 모델에서 사용되지 않는 함수로 임곗값에서 불연속점을 가지므로 미분이 불가능해 학습을 진행할 수 없다. 또한, 역전파 과정에서 데이터가 극단적으로 변경되기 때문에 적합하지 않다.

### 임계값 함수(Threshold Function)

임계값 함수는 임계값보다 크면 입력값을 그대로 전달하고, 작으면 특정 값으로 변경한다. 출력이 0 또는 1인 이진 분류 작업을 위해 신경망에서 자주 사용되는 효과적인 활성화 함수다. 하지만 입력에 대한 함수의 기울기를 계산할 수 없으므로 네트워크를 최적화하기 어려워 사용되지 않는 함수다.
$$
Threshold(x)=\left\{ \begin{array}{rcl}{x} & \mbox{if}& threshold<x \\ value & \mbox{else} & otherwise\end{array}\right.
$$

### 하이퍼볼릭 탄젠트 함수(Hyperbolic Tangent Function)

하이퍼볼릭 탄젠트 함수는 시그모이드 함수와 유사한 형태를 지니지만 출력값의 중심이 0이다. 출력값이 -1~1의 범위를 가지므로 시그모이드 함수에서 발생하지 않는 음수 값을 반환할 수 있다.
$$
Tanh(x)={e^x-e^{-x}\over e^x+e^{-x}}
$$
출력값의 범위가 더 넓고 다양한 형태로 활성화할 수 있으므로 기울기 소실이 비교적 덜 발생한다. 하지만 입력값이 4보다 큰 경우 출력값이 수렴하므로 동일하게 기울기 소실이 발생한다.

### ReLU(Rectified Linear Unit Function) 함수

ReLU 함수는 0보다 작거나 같으면 0을 반환하며, 0보다 크면 선형 함수에 값을 대입하는 구조를 갖는다. 시그모이드 함수나 하이퍼볼릭 탄젠트 함수는 출력값이 제한되어 기울기 소실이 발생하지만, ReLU 함수는 선형 함수에 대입하므로 입력값이 양수라면 출력값이 제한되지 않아 기울기 소실이 발생하지 않는다.

수식 또한 매우 간단해 순전파나 역전파 과정의 연산이 매우 빠르다. 하지만 입력값이 음수인 경우 항상 0을 반환하므로 가중치나 편향이 갱신되지 않을 수 있다. 가중치의 합이 음수가 되면, 해당 노드는 더 이상 값을 갱신하지 않아 죽은 뉴런이 된다. 딥러닝 네트워크에서 널리 사용되는 효과적인 활성화 함수이다.
$$
ReLU(x)=\left\{ \begin{array}{rcl}{x} & \mbox{if}& 0<x \\ 0 & \mbox{else} & otherwise\end{array}\right.
$$

### LeakyReLU 함수

Leaky ReLU 함수는 음수 기울기를 제어하여 죽은 뉴런 현상을 방지하기 위해 사용한다. 양수인 경우 ReLU 함수와 동일하지만, 음수인 경우 작은 값이라도 출력시켜 기울기를 갱신하게 한다. 작은 값을 출력시키면 더 넓은 범위의 패턴을 학습할 수 있어 네트워크의 성능을 향상시키는 데 도움이 될 수 있다.
$$
Leaky\ ReLU(x)=\left\{ \begin{array}{rcl}{x} & \mbox{if}& 0<x \\ negative\ slope\times x & \mbox{else} & otherwise\end{array}\right.
$$

### 소프트맥스 함수(Softmax Function)

소프트맥스 함수는 차원 벡터에서 특정 출력값이 k 번째 클래스에 속할 확률을 계산한다. 클래스에 속할 확률을 계산하는 함수이므로, 은닉층이 아닌 출력층에서 사용된다. 즉, 네트워크의 출력을 가능한 클래스에 대한 확률 분포로 매핑한다.
$$
p_k={e^{z_k}\over\sum_{i=1}^n e^{z_i}}
$$


## 순전파와 역전파

**순전파(Forward Propagation)**란 순방향 전달(Forward Pass)이라고도 하며 입력이 주어지면 신경망의 출력을 계산하는 프로세스다.입력 데이터를 기반으로 신경망을 따라 입력층부터 출력층까지 차례대로 변수를 계산하고 추론한 결과를 전달한다. 네트워크에 입력값을 전달해 순전파 연산을 진행한다. 이 과정에서 계층마다 가중치와 편향으로 계산된 값이 활성화 함수에 전달된다. 활성화 함수에서 출력값($\hat{y}$)이 계산되고 이 값을 손실 함수에 실젯값과 함께 연산해 오차를 계산한다.
$$
\hat{y}=activation(weight\times x+bias)
$$
**역전파(Back Propagation)**는 순전파 방향과 반대로 연산이 진해오딘다. 학습 과정에서 네트워크의 가중치와 편향은 예측된 출력값과 실제 출력값 사이의 오류를 최소화하기 위해 조정된다. 그러므로 순전파 과정을 통해 나온 오차를 활용해 각 계층의 가중치와 편향을 최적화한다.

역전파 과정에서는 각각의 가중치와 편향을 최적화하기 위해 연쇄 법칙을 활용한다. 새로 계산된 가중치는 최적화 알고리즘을 통해 실젯값과 예측값의 차이를 계산해 오차를 최소로 줄일 수 있는 가중치와 편향을 계산한다.

### 순전파 계산

아래 그림과 같이 2개의 계층으로 이루어진 모델이 있다. 손실 함수는 이진 교차 엔트로피이며, 최적화 함수는 확률적 경사 하강법을 사용한다. 학습률은 1로 설정하여 갱신되는 가중치와 편향을 확인하기 쉽게 하였다. 입력값($x_1,x_2$)과 실젯값($y$)에 각각 [1, 1]과 [0]이 입력됐다고 가정한다.

<img src="../assets/gitbook/images/인공지능기초/forward pass_1.png" style="zoom:40%;" />

RED : 가중합 계산, BLUE : 활성화 함수, GREEN : LOSS 계산이다.

### 역전파 계산

역전파 과정에서는 계층의 역순으로 가중치와 편향을 갱신한다. 즉, $W_5, W_6, b_3$ 를 갱신한 다음에 $W_1,W_2,W_3,W_4,b_1,b_2$ 가 갱신된다. 모델의 학습은 오차가 작아지는 방향으로 갱신돼야 하기 때문에 미분값이 0에 가까워져야 한다. 그러므로 갱신된 가중치와 편향의 기울기는 오차가 0이 되는 방향으로 진행한다. 갱신된 가중치나 편향은 위에서 계산된 기울기를 감산해 변화가 없을 때까지 반복한다. 
$$
W_n(t+1)=W_n(t)-\alpha{\partial\mathcal{L}\over\partial W_n(t)}
$$
위 식에서 $t$ 는 가중치 갱신 횟수를 의미하며, $\alpha$ 는 학습률을 의미한다. 이 수식을 활용해 지속해서 가중치를 갱신하면 ${\partial\mathcal{L}\over\partial W_n(t)}$ 은 점점 0에 가까워 진다. 이는 오차가 0에 가까워지도록 가중치를 갱신하는 방법이다. 결국 $W_n(t+1)\simeq W_n(t)$ 가 되어 학습이 완료된다. 

순전파를 통해 오차를 계산하고 역전파 과정을 통해 오차가 0이 될 수 있게 가중치를 갱신한다. 이러한 일련의 과정을 계속 반복해 학습을 진행한다.

<img src="../assets/gitbook/images/인공지능기초/renewal weight_1.png" style="zoom:40%;" />

모든 가중치와 편향을 갱신하면 학습이 1회 진행된 것으로 볼 수 있다. 갱신된 가중치와 편향으로 다음 학습을 진행한다. 학습이 진행될수록 오차가 점차 감소하게 된다. 모델은 위와 같은 풀이를 반복해 최적의 가중치와 편향을 찾아간다. 특정한 횟수에 도달할 때까지 연산을 진행하거나 오차가 일정 이하로 떨어지게 되면 학습을 종료한다. 갱신 결과를 비교하면 다음과 같다.

| $W_1$  | $W_2$  | $W_3$  | $W_4$  |  $W_5$  |  $W_6$  |  $b_1$  | $b_2$  |  $b_3$  |
| :----: | :----: | :----: | :----: | :-----: | :-----: | :-----: | :----: | :-----: |
| 0.4352 | 0.1951 | 0.3545 | 0.4835 | -0.1725 | 0.1129  | -0.1419 | 0.0439 | -0.3043 |
| 0.4514 | 0.1848 | 0.3707 | 0.4732 | -0.4452 | -0.1667 | -0.1257 | 0.0336 | -0.7196 |

모델의 계층 구조가 동일하더라도 오차 함수나 활성화 함수가 다르다면 다른 결과를 얻을 수 있다. 위의 예제는 학습률을 비교적 큰 1로 주었음에도 불구하고, 계층 1의 변화량은 계층 2와 비교했을 때 크지 않다. 현재 모델에서 사용한 활성화 함수는 시그모이드이다. 시그모이드는 출력값의 범위를 0~1 사이로 제한하기 때문에 역전파 과정에서 0에 가까운 기울기가 곱해지게 된다.

그러므로 역전파 과정에서 입력층의 방향으로 값을 전달하는 과정에서 0으로 수렴하는 문제가 발생해 성능이 떨어진다. 출력층에 가까운 계층 2는 변화량이 커지고, 입력층에 가까운 계층 1은 변화량이 미미해진다. 이러한 이유로 **깊은 모델의 은닉층에서는 시그모이드를 활성화 함수로 사용하지 않는다.**
