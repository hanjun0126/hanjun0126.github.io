---
categories: [AI, NLP]
description: 토큰화 내용 정리
tags: [NLP]
math: true
---

# 토큰화

**토큰(Token)**은 개별 단어나 문장 부호와 같은 텍스트를 의미하며 **말뭉치(Corpus)**보다 더 작은 단위다. 토큰은 텍스트의 개별 단어, 구두점 또는 기타 의미 단위일 수 있다. 토큰으로 나누는 목적은 컴퓨터가 자연어를 이해할 수 있게 나누는 과정이다. 이러한 과정을 **토큰화(Tokenization)**라고 한다. 토큰화를 위해 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어인 **토크나이저(Tokenizer)**를 사용한다. 토크나이저를 사용해 문장를 토큰화한다면 다음과 같이 표현할 수 있다.

---

입력 : '형태소 분속기를 이용해 간단하게 토큰화할 수 있다.'

결과 : ['형태소', '분석기', '를', '이용', '하', '어', '간단', '하', '게', '토큰', '화', '하', 'ㄹ', '수', '있', '다', '.']

---

일반적으로 토큰을 나누는 기준은 구축하려는 시스템이나 주어진 상황에 따라 다르다. 심지어 어떻게 토큰을 나누었느냐에 따라 시스템의 성능이나 처리 결과가 크게 달라지기도 한다. 토크나이저를 구축하는 방법은 다음과 같다.

| **정규표현식 적용**            | **정규표현식으로 특정 패턴을 식별해 텍스트를 분할한다.**     |
| ------------------------------ | ------------------------------------------------------------ |
| **어휘 사전(Vocabulary) 적용** | **사전에 정의된 단어 집합을 토큰으로 사용한다.**             |
| **공백 분할**                  | **텍스트를 공백단위로 분리해 개별 단어로 토큰화한다.**       |
| **머신러닝 활용**              | **데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용한다.** |

이 방법 중 **어휘 사전(Vocabulary, Vocab)**은 사전에 정의된 단어를 활용해 토크나이저를 구축하는 방법이다. 직접 어휘 사전을 구축하기 때문에 없는 단어나 토큰이 존재할 수 있다. 이러한 토큰을 **OOV(Out of Vocab)**라고 한다. 어휘 사전 방법은 OOV 문제를 고려해 토큰화하는 것이 중요하다. 큰 어휘 사전을 구축하면 학습 비용의 증대는 물론이고, 자칫 **차원의 저주(Curse of Dimensionality)**에 빠질 수 있다. 다시 말해 모든 토큰이나 단어를 벡터화하면 어휘 사전에 등장하는 토큰 개수만큼의 차원이 필요하고, 벡터값이 거의 모두 0 의 값을 가지는 희소(sparse) 데이터로 표현된다. 또한 희소 데이터의 표현 방법은 어휘 사전에 등장하는 출현 빈도만 고려하기 때문에 문장에서 발생한 토큰들의 순서 관계를 잘 표현하지 못한다.

---

## 단어 및 글자 토큰화

입력된 텍스트 데이터를 단어(Word)나 글자(Character) 단위로 나누는 기법으로는 단어 토큰화와 글자 토큰화가 있다. 이러한 기법을 통해 각각의 토큰은 의미를 갖는 최소 단위로 분해된다.

### 단어 토큰화

**단어 토큰화(Word Tokenization)**는 자연어 처리 분야에서 핵심적인 전처리 작업 중 하나로 텍스트 데이터를 의미 있는 단위인 단어로 분리하는 작업이다. **띄어쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 토큰화가 수행된다.** 단어 토큰화는 품사 태깅, 개체명 인식, 기계 번역 등의 작업에서 널리 사용되며 가장 일반적인 토큰화 방법이다.

```python
review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고"
tokenized = review.split()
print(tokenized)
```

**출력 결과**

```html
['현실과', '구분', '불가능한', 'cg', '시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']
```

문자열 데이터 형태는 split 메서드를 이용하여 쉽게 토큰화할 수 있다. split 메서드는 주어진 구분자를 통해 문자열을 리스트 데이터로 나눠준다. 구분자를 입력하지 않으면 공백(Whitespace)을 기준으로 나눈다.

'cg.' 와 'cg' 는 비슷한 의미가 있는 것을 알고 있지만, 단어 토큰화를 통해 만들어진 단어 사전에서는 이 두 토큰은 다른 토큰이다. 즉 'cg' 라는 토큰이 단어 사전 내에 있더라도 'cg.', 'cg는', 'cg도' 등은 OOV가 된다. 이처럼 단어 토큰화는 한국어 접사, 문장 부호, 오타 혹은 띄어쓰기 오류 등에 취약하다.

### 글자 토큰화

**글자 토큰화(Character Tokenization)**는 띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식으로, 비교적 작은 단어 사전을 구축할 수 있다는 장점이 있다. 작은 단어 사전을 사용하면 학습 시 컴퓨터 자원을 아낄 수 있으며, 전체 말뭉치를 학습할 때 각 단어를 더 자주 학습할 수 있다는 장점이 있다. 글자 토큰화는 언어 모델링과 같은 시퀀스 예측 작업에서 활용된다. 예를 들어 다음 문자를 예측하는 언어 모델링에서 글차 토큰화는 유용한 방식이다.

```python
review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
tokenized = list(review)
print(tokenized)
```

**출력 결과**

```html
['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']
```

글자 토큰화는 리스트 형태로 변환하면 쉽게 수행할 수 있다. 단어 토큰화와는 다르게 공백도 트큰으로 나뉜 것을 볼 수 있다. 영어의 경우 글자 토큰화를 진행하면 각 알파벳으로 나뉜다. 하지만 한글의 경우 하나의 글자는 여러 자음과 모음의 조합으로 이루어져 있다. 그러므로 자소 단위로 나눠서 자소 단위 토큰화를 수행한다. 책에서는 **자모(jamo)** 라이브러리를 활용했다. 자모 라이브러리는 한글 문자 및 자모 작업을 위한 한글 음절 분해 및 합성 라이브러리다. 

컴퓨터가 한글을 인코딩하는 방식은 크게 조합형과 완성형으로 나눌 수 있다. 조합형은 글자를 자모 단위로 나눠 인코딩한 뒤 이를 조합해 한글을 표현한다. 완성형은 조합된 글자 자체에 값을 부여해 인코딩하는 방식이다. 그러므로 h2j 함수는 완성형으로 입력된 한글을 조합형 한글로 변화한다. j2hcj 함수는 조합형 한글 문자열을 자소 단위로 나눠 반환하는 함수다. 조합형 한글로 입렫뇓 문자열은 초성, 중성, 종성으로 나뉜다. 이 함수를 통해 완성형 한글을 쉽게 자소 단위로 나눌 수 있다.

```python
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```

**출력 결과** 

```html
['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']
```

자모 변환 함수와 한글 호환성 자모 변환 함수를 활용해 자소 단위로 분리했다. 이렇게 글자 단위로 트큰화하면 단어 단위로 토큰화하는 것에 비해 적은 크기의 단어 사전을 구축할 수 있다. 앞서 단어 토큰화의 약점이었던 접사와 문장 부호의 의미를 학습할 수 있다. 또, 작은 크기의 단어 사전으로도 OOV를 획기적으로 줄일 수 있다. 

하지만 개별 토큰은 아무런 의미가 없으므로 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야 한다. 토큰 조합 방식을 사용해 문장 생성이나 **개체명 인식(Name Entity Recognition)** 등을 구현할 경우, 다의어나 동음이의어가 많은 도메인에서 구별하는 것이 어려울 수 있다. 또한, 모델 입력 시퀀스(sequence)의 길이가 길어질수록 연산량이 증가한다는 단점이 있다.



---

## 형태소 토큰화

**형태소 토큰화(Morpheme Tokenization)**란 텍스트를 형태소 단위로 나누는 토큰화 방법으로 언어의 문법과 구조를 고려해 단어를 분리하고 이를 의미 있는 단위로 분류하는 작업이다. 형태소 토큰화는 한국어와 같이 교착어(Agglutinative Language)인 언어에서 중요하게 수행된다. 한국어는 대부분의 언어와 달리 각 단어가 띄어쓰기로 구분되지 않는다. 한국어는 어근에 다양한 접사와 조사가 조합되어 하나의 낱말을 이루므로 각 형태소를 적절히 구분해 처리해야 한다.

예를 들어 '그는 나에게 인사를 했다'라는 문장을 보면 '그는'은 '그'라는 단어와 '는'이라는 조사가 결합해 하나의 단어로 이루어져 있고, '나'라는 단어와 '에게'라는 조사가 결합해 하나의 단어로 이루어져 있다. 이처럼 '그', '-는', '나', '-에게' 등과 같이 실제로 의미를 가지고 있는 최소의 단위를 형태소(Morpheme)라고 한다. 형태소는 크게 스스로 의미를 가지고 있는 **자립 형태소(Free Morpheme)**와 스스로 의미를 갖지 못하고 다른 형태소와 조합되어 사용되는 **의존 형태소(Bound Morpheme)**로 구분된다.

자립 형태소는 단어의 기본이 되는 형태소로서, 명사, 동사, 형용사와 같은 단어를 이루는 기본 단위다. 반면, 의존 형태소는 자립 형태소와 함께 조합되어 문장에서 특정한 역할을 수행하며, 조사, 어미, 접두사, 접미사 등이 해당된다.  이러한 형태소 분석을 통해 문장 내 각 형태소의 역할을 파악할 수 있으며, 이를 바탕으로 문장을 이해하고 처리할 수 있다.

### 형태소 어휘 사전

**형태소 어휘 사전(Morpheme Vocabulary)**은 자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전을 말한다. 단어가 어떤 형태소들의 조합으로 이루어져 있는지에 대한 정보를 담고 있어, 형태소 분석 작업에서 매우 중요한 역할을 한다.

일반적으로 형태소 어휘 사전에는 각 형태소가 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보도 함께 제공된다. 텍스트 데이터를 형태소 분석하여 각 형태소에 해당하는 **품사(Part Of Speech, POS)**를 태깅하는 작업을 **품사 태깅(POS Tagging)**이라고 한다. 이를 통해 자연어 처리 분야에서 문맥을 고려할 수 있어 더욱 정확한 분석이 가능해진다.

### KoNLPy

**KoNLPy** 는 한국어 자연어 처리를 위해 개발된 라이브러리로 명사 추출, 형태소 분석, 품사 태깅 등의 기능을 제공한다. 텍스트 데이터를 전처리하고 분석하기 위한 다양한 도구와 함수를 제공해 텍스트 마이닝, 감성 분석, 토픽 모델링 등 다양한 NLP 작업에서 사용한다.

KoNLPy 는 **Okt(Open Korean Text), 꼬꼬마(Kkma), 코모란(Komoran), 한나눔(Hannanum), 메캅(Mecab)** 등의 다양한 형태소 분석기를 지원한다.

```python
from konlpy.tag import Okt

okt = Okt()

sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = okt.nouns(sentence)					# 명사 추출
phrases = okt.phrases(sentence)			# 구문 추출
morphs = okt.morphs(sentence)				# 형태소 추출
pos = okt.pos(sentence)							# 품사 태깅

print("명사 추출 :", nouns)
print("구 추출 :", phrases)
print("형태소 추출 :", morphs)
print("품사 태깅:", pos)
```

**출력 결과**

```html
명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']
구 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']
형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']
품사 태깅: [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]
```

Okt 객체는 문장을 입력받아 명사, 구, 형태소, 품사 등의 정보를 추출하는 여러가지 메서드를 제공한다. 명사 추출, 구문 추출, 형태소 추출은 입력된 문장에서 각각 명사, 어절 구 단위, 형태소만 추출해 리스트를 반환한다. 품사 태깅은 입력된 문장에서 각 단어에 대한 품사 정보를 추출하여 (형태소, 품사) 형태의 튜플로 구성된 리스트를 반환한다.

```python
from konlpy.tag import Kkma

kkma = Kkma()

sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = kkma.nouns(sentence)
sentences = kkma.sentences(sentence)
morphs = kkma.morphs(sentence)
pos = kkma.pos(sentence)

print("명사 추출 :", nouns)
print("문장 추출 :", sentences)
print("형태소 추출 :", morphs)
print("품사 태깅:", pos)
```

**출력 결과**

```html
명사 추출 : ['무엇', '상상', '수', '사람', '무엇']
문장 추출 : ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']
형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']
품사 태깅: [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]
```

꼬꼬마는 Kkma 클래스를 통해 명사, 문장, 형태소, 품사를 추출할 수 있다.꼬꼬마는 구문 추출 기능은 지원하지 않지만, **문장 추출(kkma.sentences)** 기능을 제공한다. 동일한 메서드라도 형태소 분석기의 특징에 따라 다른 결과가 나타난다. 예를 들어 명사 추출의 경우 Okt에서 반환하던 '낼'과 '수'를 반환하지 않았으며, 형태소 추출의 경우 '할'을 더 분리해 '하', 'ㄹ'로 반환한 것을 확인할 수 있다.

품사 태깅 역시 상이한 결과를 제공한다. Okt는 총 19개의 품사를 구분해 반환하는 한편, 꼬꼬마는 더 세분화해 56개의 품사로 태깅한다. 태깅하는 품사의 수가 많으면 더 자세한 단위로 분석이 가능하지만, 품사 태깅에 소요되는 시간도 길어지며, 더 많은 품사로 분리해 모델의 성능이 저하될 수 도 있다.

그러므로 시스템의 목적과 환경에 맞는 적절한 형태소 분석기를 사용해야 한다.



### NLTK

**NLTK(Natural Language Toolkit)**는 자연어 처리를 위해 개발된 라이브러리로, 토큰화, 형태소 분석, 구문 분석, 개체명 인식, 감성 분석 등과 같은 기능을 제공한다. NLTK 는 주로 영어 자연어 처리를 위해 개발됐지만, 네덜란드어, 프랑스어, 독일어 등과 같은 다양한 언어의 자연어 처리를 위한 데이터와 모델을 제공한다.

NLTK 라이브러리를 활용해 토큰화나 품사 태깅 작업을 하기 위해서는 해당 작업을 수행할 수 있는 패키지나 모델을 다운로드해야 한다. 책에서는 **Punkt** 모델과 **Averaged Perceptron Tagger** 모델을 활용해 토큰화 및 품사 태깅 작업을 수행한다. 두 모델 모두 트리뱅크(Treebank)라는 대규모의 영어 말뭉치를 기반으로 학습됐다. Punkt 모델은 통계 기반 모델이며, Averaged Perceptron Tagger 는 퍼셉트론을 기반으로 품사 태깅을 수행한다.

```python
import nltk

nltk.download("punkt")
nltk.download("punkt_tab")
nltk.download("averaged_perceptron_tagger")
nltk.download('averaged_perceptron_tagger_eng')
```

사전에 모델을 설치해야 한다.

```python
from nltk import tokenize

sentence = "Those who can imagine anything, can create the impossible."

word_tokens = tokenize.word_tokenize(sentence)
sent_tokens = tokenize.sent_tokenize(sentence)

print(word_tokens)
print(sent_tokens)
```

**출력 결과**

```html
['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']
['Those who can imagine anything, can create the impossible.']
```

영문 토큰화는 사전에 설치한 punkt 모델을 기반으로 단어나 문장을 토큰화한다. **단어 토크나이저(word_tokenize)**는 문장을 입력받아 공백을 기준으로 단어를 분리하고, 구두점 등을 처리해 각각의 단어(token)를 추출해 리스트로 반환한다. **문장 토크나이저(sent_tokenize)**는 문장을 입력받아 마침표(.), 느낌표(!), 물음표(?) 등의 구두점을 기준으로 문장을 분리해 리스트로 반환한다.

```python
from nltk import tag
from nltk import tokenize

sentence = "Those who can imagine anything, can create the impossible."

word_tokens = tokenize.word_tokenize(sentence)
pos = tag.pos_tag(word_tokens)

print(pos)
```

**출력 결과**

```html
[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]
```

NLTK 라이브러리의 **품사 태깅(pos_tag)** 메서드는 토큰화된 문장에서 품사 태깅을 수행한다. 품사 태깅을 수행하기 위해서는 토큰화된 단어들이 들어가야 한다.

### spaCy

**spaCy**는 사이썬(Cython) 기반으로 개발된 오픈 소스 라이브러리로서, NLTK 라이브러리와 마찬가지로 자연어 처리를 위한 기능을 제공한다. NLTK 라이브러리와의 주요한 차이점은 빠른 속도와 높은 정확도를 목표로 하는 머신러닝 기반의 자연어 처리 라이브러리라는 점이다. NLTK 는 학습 목적으로 자연어 처리에 대한 다양한 알고리즘과 예제를 제공하는 반면, spaCy는 효울적인 처리 속도와 높은 정확도를 제공하는 것을 목표로 한다. 그러므로 NLTK 에서 사용하는 모델보다 더 크고 복잡하며 더 많은 리소스로 요구한다.

spaCy 는 GPU 가속을 비롯해 영어, 프랑스어, 한국어, 일본어 등을 비롯해 24개 이상의 언어로 사전 학습된 모델을 제공한다.

```python
import spacy

nlp = spacy.load("en_core_web_sm")
sentence = "Those who can imagine anything, can create the impossible."
doc = nlp(sentence)

for token in doc:
    print(f"[{token.pos_:5} - {token.tag_:3}] : {token.text}")
```

**출력 결과**

```html
[PRON  - DT ] : Those
[PRON  - WP ] : who
[AUX   - MD ] : can
[VERB  - VB ] : imagine
[PRON  - NN ] : anything
[PUNCT - ,  ] : ,
[AUX   - MD ] : can
[VERB  - VB ] : create
[DET   - DT ] : the
[ADJ   - JJ ] : impossible
[PUNCT - .  ] : .
```

spaCy 는 사전 학습된 모델을 기반으로 처리하기 때문에 spaCy 모델 불러오기 함수(load)를 통해 모델을 설정할 수 있다. spaCy 는 **객체 지향적(Object oriented)**으로 구현돼 처리한 결과를 doc 객체에 저장한다. doc 객체는 다시 여러 token 객체로 이뤄져 있으며, 이 token 객체에 대한 정보를 기반으로 다양한 자연어 처리 작업을 수행한다. nlp 인스턴스에 문장을 입력하면 doc 객체가 반환되며 token 객체의 tag_ 나 pos_ 와 같은 속성에 접근해 값을 확인할 수 있다. 



---

## 하위 단어 토큰화

형태소 분석기는 전문용어나 고유어가 많은 데이터를 처리할 때 약점을 보인다. 즉, 형태소 분석기는 모르는 단어를 적절한 단위로 나누는 것에 취약하며, 이는 잠재적으로 어휘 사전의 크기를 크게 만들고 OOV 에 대응하기 어렵게 만든다. 현대 자연어 처리에서는 신조어의 발생, 오탈자, 축약어 등을 고려해야 하기 때문에 분석할 단어의 양이 많아져 어려움을 겪는다. 이를 해결하기 위한 방법 중 하나로 **하위 단어 토큰화(Subword Tokenization)**가 있다. 하위 단어 토큰화란 하나의 단어가 빈번하게 사용되는 **하위 단어(Subword)**의 조합으로 나누어 토큰화하는 방법이다. 예를 들어 'Reinforcement' 라는 단어는 길이가 비교적 길어 처리가 어려울 수 있다. 하위 단어 토큰화를 적용한다면 'Rein', 'force', 'ment' 등으로 나눠 처리할 수 있다.

하위 단어 토큰화를 적용하면 단어의 길이를 줄일 수 있어서 처리 속도가 빨라질 뿐만 아니라, OOV 문제, 신조어, 은어, 고유어 등으로 인한 문제를 완화할 수 있다. 하위 단어 토큰화 방법으로는 **바이트 페어 인코딩, 워드피스, 유니그램** 모델 등이 있다.

### 바이트 페어 인코딩

**바이트 페어 인코딩(Byte Pair Encoding. BPE)**이란 다이그램 코딩(Digram Coding) 이라고도 하며 하위 단어 토큰화의 한 종류다. 텍스트 데이터에서 가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘으로 초기에는 데이터 압축을 위해 개발됐으나, 자연어 처리 분야에서 하위 단어 토큰화를 위한 방법으로 사용된다.

이 알고리즘은 연속된 글자 쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 조합 탐지와 부호화를 반복하며 이과정에서 자주 등장하는 단어는 하나의 토큰으로 토큰화되고, 덜 등장하는 단어는 여러 토큰의 조합으로 표현된다.

원문 : abracadabra &rarr; Step #1: AracadAra &rarr; Step #2: ABcadAB &rarr; Step #3: CcadC

바이트 페어 인코딩은 입력 데이터에서 가장 많이 등장한 글자의 빈도수를 측정하고, 가장 빈도수가 높은 글자 쌍을 탐색한다. 현재 원문 'abracadabra' 에서 'ab' 글자 쌍이 가장 빈도수가 높으므로 입력 데이터에 없는 새로운 글자인 'A' 로 치환한다. 동일한 방법으로 다시 탐색을 수행하면 'AracadAra' 에서 'ra' 글자 쌍이 가장 빈도수가 높으므로 'B' 로 치환한다. 치환된 데이터는 'ABcadAB' 로 아직 'AB' 라는 글자 쌍이 존재한다. 치환된 글자도 글자 쌍에 포함될 수 있으므로 'AB' 를 'C' 로 치환한다.

더 이상 치환할 수 있는 글자 쌍이 존재하지 않으므로 입력 데이터를 더 이상 압축할 수 없다. 그러므로 원문은 'CcadC' 로 압축된다. 토크나이저로써 바이트 페어 인코딩은 자주 등장하는 글자 쌍을 찾아 치환하는 대신 어휘 사전에 추가한다.

---

이번에는 말뭉치에서 바이트 페어 인코딩을 적용해 본다. 말뭉치에서 각 단어가 등장한 빈도를 계산해 다음과 같은 빈도 사전과 어휘 사전을 만들었다고 가정해보자.

빈도 사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)				어휘 사전: ['low', 'lower', 'newest', 'widest']

빈도 사전을 보면 말뭉치에 각각의 단어가 5번, 2번, 6번, 3번 등장했다는 것을 알 수 있다. 빈도 사전을 바이트 페어 인코딩으로 재구성한다고 가정해보자. 이 알고리즘을 적용하기 위해 빈도 사전 내 모든 단어를 글자 단위로 나눈다.

빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)

어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']

빈도 사전을 기준으로 가장 자주 등장한 글자 쌍을 찾는다. 빈도 사전에서 'e', 's' 쌍이 총 9번으로 가장 많이 등장했다. 그러므로 빈도 사전에서 'e' 와 's' 를 'es' 로 병합하고 어휘 사전에 'es' 를 추가한다.

빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'es', 't', 6), ('w', 'i', 'd', 'es', 't', 3)

어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es']

동일한 과정을 다시 반복하면서 10번 반복했다고 가정하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.

빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)

어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest']

BPE 알고리즘을 사용해 말뭉치에서 자주 등장하는 글자 쌍을 찾아 어휘 사전을 구축했다. 따라서 'newer', 'wider', 'lowest', 와 같이 기존 말뭉치에 등장하지 않았던 단어가 입력되더라도 OOV 로 처리되지 않고 기존 어휘 사전을 참고해 'new e r', 'wid e r', 'low est' 와 같이 토큰화할 수 있다.

---

### 센텐스피스

**센텐스피스(Sentencepiece)**<sup> 라이브러리는 구글에서 개발한 오픈소스 하위 단어 토크나이저 라이브러리다. 바이트 페어 인코딩과 유사한 알고리즘을 사용해 입력 데이터를 토큰화하고 단어 사전을 생성한다. 또한 워드피스, 유니코드 기반의 다양한 알고리즘을 지원하며 사용자가 직접 설정할 수 있는 하이퍼파라미터들을 제공해 세밀한 토크나이징 기능을 제공한다. **코포라(Korpora)** 라이브러리는 국립국어원이나 AI Hub 에서 제공하는 말뭉치 데이터를 쉽게 사용할 수 있게 제공하는 오픈소스 라이브러리다. 파이썬에서 쉽게 사용할 수 있게 API 가 제공된다.

```python
from Korpora import Korpora

corpus = Korpora.load("korean_petitions")
petitions = corpus.get_all_texts()
with open("../datasets/corpus.txt", "w", encoding="utf-8") as f:
    for petition in petitions:
        f.write(petition + "\n")
```

Korpora 라이브러리를 사용하여 청원 데이터를 하나의 텍스트 파일로 저장한다.

```python
from sentencepiece import SentencePieceTrainer

SentencePieceTrainer.Train(
    "--input=../datasets/corpus.txt\
    --model_prefix=petition_bpe\
    --vocab_size=8000 model_type=bpe"
)
```

센텐스피스 라이브러리는 SentencePieceTrainer 클래스의 Train 메서드로 토크나이저 모델을 학습할 수 있다. 토크나이저 모델 학습이 완료되면 petition_bpe.model 파일과 petition_bpe.vocab 파일이 생성된다. model 파일은 학습된 토크나이저가 저장된 파일이며, vocab 파일은 어휘 사전이 저장된 파일이다.

```python
from sentencepiece import SentencePieceProcessor

tokenizer = SentencePieceProcessor()
tokenizer.load("petition_bpe.model")

sentence = "안녕하세요, 토크나이저가 잘 학습되었군요!"
sentences = ["이렇게 입력값을 리스트로 받아서", "쉽게 토크나이저를 사용할 수 있답니다"]

tokenized_sentence = tokenizer.encode_as_pieces(sentence)
tokenized_sentences = tokenizer.encode_as_pieces(sentences)

print("단일 문장 토큰화 :", tokenized_sentence)
print("여러 문장 토큰화 :", tokenized_sentences)

encoded_sentence = tokenizer.encode_as_ids(sentence)
encoded_sentences = tokenizer.encode_as_ids(sentences)
print("단일 문장 정수 인코딩 :", encoded_sentence)
print("여러 문장 정수 인코딩 :", encoded_sentences)

decode_ids = tokenizer.decode_ids(encoded_sentences)
decode_pieces = tokenizer.decode_pieces(encoded_sentences)
print("정수 인코딩에서 문장 변환 :", decode_ids)
print("하위 단어 토큰에서 문장 변환 :", decode_pieces)
```

**출력 결과**

```html
단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '되었', '군요', '!']
여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]
단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]
여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]
정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']
하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']
```

센텐스피스 토크나이저 모델은 SentencePieceProcessor 클래스를 통해 학습된 모델을 불러올 수 있다. 토크나이저 모델 불러오기(tokenizer.load) 메서드를 통해 petition_bpe.model 모델을 불러온다. encode_as_pieces 메서드는 문장을 토큰화하며, encode_as_ids 메서드는 토큰을 정수로 인코딩해 제공한다. 이 정수 데이터는 어휘 사전의 토큰에 매핑된 ID 값을 의미한다. 이 ID 값을 활용해 자연어 처리 모델을 구축한다. 토크나이저 모델이나 자연어 처리 모델에서 나온 정수는 decode_ids 메서드나 decode_pieces 메서드를 통해 문자열 데이터로 변환할 수 있다.

어휘 사전 데이터를 불러와 딕셔너리 형태로 매핑하는 방법이다.

```python
from sentencepiece import SentencePieceProcessor

tokenizer = SentencePieceProcessor()
tokenizer.load("petition_bpe.model")

vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}
print(list(vocab.items())[:5])
print("vocab size :", len(vocab))
```

**출력 결과**

```html
[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]
vocab size : 8000
```

get_piece_size 메서드는 센텐스피스 모델에서 생성된 하위 단어의 개수를 반환하며, id_to_piece 메서드는 정숫값을 하위 단어로 변환하는 메서드다. 그러므로 하위 단어의 개수만큼 반복해 하위 단어 딕셔너리를 구성한다. 

토큰 딕셔너리에는 <unk>, </s>, <s> 가 있고, 각각 OOV 로 발생 시 매핑되는 토큰과, eos, sos 토큰이다.



### 워드피스

**워드피스(Wordpiece)** 토크나이저는 바이트 페어 인코딩 토크나이저와 유사한 방법으로 학습되지만, 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합한다. 워드피스는 학습 과정에서 확률적인 정보를 사용한다. 모델이 새로운 하위 단어를 생성할 때 이전 하위 단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어를 선택한다. 이렇게 선택된 하위 단어는 이후에 더 높은 확률로 선택될 가능성이 높으며, 이를 통해 모델이 좀 더 정확한 하위 단어로 분리할 수 있다. 각 글자 쌍에 대한 점수는 다음 수식과 같이 계산된다.
$$
score = {f(x,y)\over f(x),f(y)}
$$
$f$ 는 빈도(frequency)를 나타내는 함수이며, $x$ 와 $y$ 는 병합하려는 하위 단어를 의미한다. 그러므로 $f(x,y)$ 는 $x$ 와 $y$ 가 조합된 글자 쌍의 빈도를 의미한다. 즉, $xy$ 글자 쌍의 빈도가 된다. 그러므로 $score$ 는 $x$ 와 $y$ 를 병합하는 것이 적절한지를 판단하기 위한 점수가 된다.

---

바이트 페어 인코딩의 예시로 사용한 말뭉치를 활용해 빈도 사전과 어휘 사전을 구축했다고 가정해 보자. 워드피스도 바이트 페어 인코딩과 마찬가지로 빈도 사전 내의 모든 단어를 글자 단위로 나눈다.

빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)

어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']

가장 빈번하게 등장한 쌍은 9번 등장한 'e' 와 's' 다. 하지만 'e' 는  17번, 's' 는 9번 등장하므로 점수는 ${9\over 17*9}\simeq 0.06$ 이 된다. 'i' 와 'd' 쌍은 3번밖에 등장하므로 점수는 ${3\over 3*3}\simeq 0.33$ 이 된다. 따라서 'e' 와 's' 쌍 대신 'i' 와 'd' 쌍을 병합한다.

빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'id', 'e', 's', 't', 3)

어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'id']

동일한 과정을 반복해 각 글자 쌍에 대한 점수를 계산한다. 워드피스 토크나이저는 바이트 페어 인코딩 토크나이저와 마찬가지로 위 과정을 반복해 연속된 글자 쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 학습한다.

---

### 토크나이저스

**토크나이저스(Tokenizers) 라이브러리의 워드피스 API 를 이용하면 쉽고 빠르게 토크나이저를 구현하고 학습할 수 있다. 토크나이저스 라이브러리는 **정규화(Normalization)**와 **사전 토큰화(Pre-tokenization)**를 제공한다. 정규화는 일관된 형식으로 텍스트를 표준화하고 모호한 경우를 방지하기 위해 일부 문자를 대체하거나 제거하는 등의 작업을 수행한다. 불필요한 공백 제거, 대소문자 변환, 유니코드 정규화, 구두점 처리, 특수 문자 처리 등을 제공한다. 사전 토큰화는 입력 문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능을 제공한다. 공백 혹은 구두점을 기준으로 입력 문장을 나눠 텍스트 데이터를 효율적으로 처리하고 모델의 성능을 향상시킬 수 있다.

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.normalizers import Sequence, NFD, Lowercase
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(WordPiece())
tokenizer.normalizer = Sequence([NFD(), Lowercase()])
tokenizer.pre_tokenizer = Whitespace()

tokenizer.train(["../datasets/corpus.txt"])
tokenizer.save("../models/petition_wordpiece.json")
```

토크나이저스 라이브러리의 토크나이저(Tokenizer)로 워드피스(WordPiece) 모델을 불러온다. 모델을 불러왔다면 정규화 방식과 사전 토큰화 방식을 설정한다. 정규화 방식은 normalizers 모듈에 포함된 클래스를 불러와 시퀀스 형식으로 인스턴스를 전달한다. 정규화 방식은 NFD 유니코드 정규화(NFD), 소문자 변환(Lowercase)을 사용하였다. 사전 토큰화 방식도 pre_tokenizers 모듈에 포함된 클래스를 불러와 적용하였다.

```python
from tokenizers import Tokenizer
from tokenizers.decoders import WordPiece as WordPieceDecoder

tokenizer = Tokenizer.from_file("../models/petition_wordpiece.json")
tokenizer.decoder = WordPieceDecoder()

sentence = "안녕하세요, 워드피스 토크나이저 입니다"
sentences = ["입력값을 리스트로 전달해", "쉽게 토크나이저를 사용할 수 있습니다."]

encoded_sentence = tokenizer.encode(sentence)
encoded_sentences = tokenizer.encode_batch(sentences)

print("인코더 형식 :", type(encoded_sentence))

print("단일 문장 토큰화 :", encoded_sentence.tokens)
print("여러 문장 토큰화 :", [enc.tokens for enc in encoded_sentences])

print("단일 문장 정수 인코딩 :", encoded_sentence.ids)
print("여러 문장 정수 인코딩 :", [enc.ids for enc in encoded_sentences])

print("정수 인코딩에서 문장 변환 :", tokenizer.decode(encoded_sentence.ids))
```

**출력 결과**

```html
인코더 형식 : <class 'tokenizers.Encoding'>
단일 문장 토큰화 : ['안녕하세요', ',', '워', '##드', '##피', '##스', '토', '##크', '##나이', '##저', '입니다']
여러 문장 토큰화 : [['입력', '##값을', '리스트', '##로', '전달', '##해'], ['쉽게', '토', '##크', '##나이', '##저', '##를', '사용할', '수', '있습니다', '.']]
단일 문장 정수 인코딩 : [8760, 11, 10398, 7638, 7955, 7512, 8693, 8415, 16269, 7536, 8150]
여러 문장 정수 인코딩 : [[19643, 13834, 28119, 7495, 11589, 7502], [9739, 8693, 8415, 16269, 7536, 7510, 14129, 7562, 7698, 13]]
정수 인코딩에서 문장 변환 : 안녕하세요, 워드피스 토크나이저 입니다
```

워드피스 토큰화를 수행하기 위해 모델 결과가 저장된 petition_wordpiece.json 파일을 불러와 Tokenizer 객체를 생성한다. 그런 다음 WordPieceDecoder() 를 사용해 Tokenizer 의 디코더를 워드피스 디코더로 설정한다. 인코딩 메서드로 문장을 토큰화할 수 있으며, 인코딩 배치 메서드로 여러 문장을 한 번에 토큰화할 수 있다. 인코딩 메서드와 인코딩 배치 메서드 모두 워드피스 토크나이저를 통해 문장을 토큰화하고 각 토큰의 색인 번호를 반환한다. 토큰화된 데이터는 토큰 속성을 통해 값을 확인할 수 있으며, 토큰 정수(ids) 속성으로 인코딩된 문장의 ID 값을 출력할 수 있다. 정수를 다시 문장으로 변환하는 경우 디코딩 메서드를 통해 정수 인코딩된 결과를 다시 문장으로 디코딩해 출력할 수 있다.

