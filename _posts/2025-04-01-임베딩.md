---
categories: [NLP]
description: 임베딩 내용 정리
tags: [NLP]
math: true
---

# 임베딩

토큰화만으로는 모델을 학습할 수 없다. 컴퓨터는 텍스트 자체를 이해할 수 없으므로 텍스트를 숫자로 변환하는 **텍스트 벡터화(Text Vectorization)** 과정이 필요하다. 텍스트 벡터화란 **텍스트를 숫자로 변환하는 과정을 의미한다.** 기초적인 텍스트 벡터화로는 **원-핫 인코딩(One-Hot Encoding)**, **빈도 벡터화(Count Vectorization)** 등이 있다. 

원핫 인코딩이란 문서에 등장하는 **각 단어를 고유한 색인 값으로 매핑한 후, 해당 색인 위치를 1로 표시하고 나머지 위치는 모두 0으로 표시하는 방식**이다.

|   색인   |  0   |  1   |   2    |    3    |
| :------: | :--: | :--: | :----: | :-----: |
| **토큰** |  I   | like | apples | bananas |

위와 같이 'I like apples', 'I like bananas' 문장을 띄어쓰기 기준으로 토큰화하고 단어를 고유한 색인 값으로 매핑하였을 때, 두 문장은 각각 [1, 1, 1, 0], [1, 1, 0, 1] 과 같이 표현된다.

빈도 벡터화는 **문서에서 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방식**이다. 

이러한 방법은 단어나 문장을 벡터 형태로 변환하기 쉽고 간단하다는 장점이 있지만, **벡터의 희소성이 크다는 단점**이 있다. 또한 텍스트의 벡터가 입력 텍스트의 의미를 내포하고 있지 않으므로 두 문장이 의미적으로 유사하다고 해도 벡터가 유사하게 나타나지 않을 수 있다. 말뭉치 내에 존재하는 토큰의 개수만큼의 벡터 차원을 가져야 하지만, 입력 문장 내에 존재하는 토큰의 수는 그에 비해 현저히 적기 때문에 컴퓨팅 비용의 증가와 차원의 저주와 같은 문제를 겪을 수 있다. 또한 텍스트의 벡터가 입력 텍스트의 의미를 내포하고 있지 않으므로 두 문장이 의미적으로 유사하다고 해도 벡터가 유사하게 나타나지 않을 수 있다. 이러한 문제를 해결하기 위해 워드 투 벡터(Word2Vec)나 패스트 텍스트(fastText) 등과 같은 단어의 의미를 학습해 표현하는 **워드 임베딩(Word Embedding)** 기법을 사용한다.

워드 임베딩 기법은 **단어를 고정된 길이의 실수 벡터로 표현하는 방법**으로, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론한다. 고정된 임베딩을 학습하기 때문에 다의어나 문맥 정보를 다루기 어렵다는 단점이 있어 인공 신경망을 활용해 **동적 임베딩(Dynamic Embedding)** 기법을 사용한다.

---

## 언어 모델

**언어 모델(Language Model)**이란 입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델을 의미한다. 이를 위해 주어진 문장을 바탕으로 문맥을 이해하고, 문장 구성에 대한 예측을 수행한다. 주어진 문장 뒤에 나올 수 있는 문장은 매우 다양하기 때문에 완성된 문장 단위로 확률을 계산하는 것은 어려운 일이다. 이러한 문제를 해결하기 위해 문장 전체를 예측하는 방법 대신에 하나의 토큰 단위로 예측하는 방법인 자기회귀 언어 모델이 고안됐다.

### 자기회귀 언어 모델

**자기회기 언어 모델(Autoregressive Language Model)**은 입력된 문장들의 조건부 확률을 이용해 다음에 올 단어를 예측한다. 즉, 언어 모델에서 조건부 확률은 이전 단어들의 시퀀스가 주어졌을 때, 다음 단어의 확률을 계산하는 것을 의미한다.





### N-gram

**N-gram** 모델은 텍스트에서 N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정한다. 모델은 입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석한다. 이때, 연속된 N개의 단어를 하나의 단위로 취급하여 추론하는 모델이며, N이 1일 때는 **유니그램(Unigram)**, N이 2일 때는 **바이그램(Bigram)**, N이 3일 때는 **트라이그램(Trigram)**으로 부른다. N이 4 이상이면 N-gram 이라고 부른다.

N-gram 언어 모델은 모든 토큰을 사용하지 않고 N-1 개의 토큰만을 고려해 확률을 계산한다. 따라서 각 N-gram 언어 모델에서 $t$ 번째 토큰의 조건부 확률을 계산하는 수식은 다음과 같다.
$$
P(w_t|w_{t-1},w_{t-2},\cdot\cdot\cdot,w_{t-N+1})
$$
$w_t$ 는 예측하려는 단어, $w_{t-1}$ 부터 $w_{t-N+1}$ 까지는 예측에 사용되는 이전 단어들을 의미한다. 이전 단어들의 개수를 결정하는 $N$ 의 값을 조정하여 N-gram 모델의 성능을 조절할 수 있다.

```python
import nltk

# 파이썬 코드로 구현한 N-gram
def ngrams(sentence, n):
    words = sentence.split()
    ngrams = zip(*[words[i:] for i in range(n)])
    return list(ngrams)

sentence = "안녕하세요 만나서 진심으로 반가워요"

unigram = ngrams(sentence, 1)
bigram = ngrams(sentence, 2)
trigram = ngrams(sentence, 3)

print(unigram)
print(bigram)
print(trigram)

# nltk 라이브러리에서 지원하는 N-gram
unigram = nltk.ngrams(sentence.split(), 1)
bigram = nltk.ngrams(sentence.split(), 2)
trigram = nltk.ngrams(sentence.split(), 3)

print(list(unigram))
print(list(bigram))
print(list(trigram))
```

**출력 결과**

```html
[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]
[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]
[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]
[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]
[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]
[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]
```

N-gram 은 작은 규모의 데이터세트에서 연속된 문자열 패턴을 분석하는데 큰 효과를 보인다. 또한 관용적 표현 분석에도 활용된다. N-gram 은 시퀀스에서 연속된 n개의 단어를 추출하므로 단어의 순서가 중요한 자연어 처리 작업 및 문자열 패턴 분석에 활용된다.

### TF-IDF

**TF-IDF(Term Frequency-Inverse Document Frequency)**란 텍스트 문서에서 특정 단어의 중요도를 계산하는 방법으로, 문장 내에서 단어의 중요도를 평가하는 데 사용되는 통계적인 가중치를 의미한다. 즉, TF-IDF 는 **BoW(Bag-of-Words)**에 가중치를 부여하는 방법이다.

BoW는 문서나 문장을 단어의 집합으로 표현하는 방법으로, 문서나 문장에 등장하는 단어의 중복을 허용해 빈도를 기록한다. 원핫 인코딩은 단어의 등장 여부를 판별해 0과 1로 표현하는 방식이지만, BoW는 등장 빈도를 고려해 표현한다. 예를 들어, ['This movie is famous movie', 'I like this movie', 'I don't like this movie'] 말뭉치를 BoW로 벡터화하면 다음과 같다.

|                            |  I   | like | this | movie | don't | famous |  is  |
| :------------------------: | :--: | :--: | :--: | :---: | :---: | :----: | :--: |
| This movie is famous movie |  0   |  0   |  1   |   2   |   0   |   1    |  1   |
|     I like this movie      |  1   |  1   |  1   |   1   |   0   |   0    |  0   |
|  I don't like this movie   |  1   |  1   |  1   |   1   |   1   |   0    |  0   |

BoW를 이용해 벡터화하는 경우 모든 단어는 동일한 가중치를 갖는다. BoW 벡터를 활용해 영화리뷰의 긍/부정 분류 모델을 만든다고 가정한다면 높은 성능을 얻기는 어렵다. 'I like this movie'와 'I don't like this movie' 문장은 don't 라는 단어가 문장 분류에 결정적인 역할을 한다. 하지만 don't 라는 단어는 자주 등장하지 않으므로 토큰화나 분류 모델 진행시 해당 데이터가 무시될 수 있다.

<p style="font-size:125%">단어 빈도</p>

**단어 빈도(Term Frequency, TF)**란 문서 내에서 특정 단어의 빈도수를 나타내는 값이다. 예를 들어 3개의 문서에서 'movie'라는 단어가 4번 등장한다면 해당 단어의 TF 값은 4가 된다.
$$
TF(t,d) = count(t,d)
$$
앞선 BoW 벡터 표현 방법과 같이 문서 내에서 단어가 등장한 빈도수를 계산하며, 해당 단어의 상대적인 중요도를 측정하는 데 사용된다. TF 값이 높을수록 해당 단어가 특정 문서에서 중요한 역할을 한다고 생각할 수도 있지만, 단어 자체가 특정 문서 내에서 자주 사용되는 단어이므로 전문 용어나 관용어로 간주할 수 있다. TF는 단순히 단어의 등장 빈도수를 계산하기 때문에 문서의 길이가 길어질수록 해당 단어의 TF 값도 높아질 수 있다.

<p style="font-size:125%">문서 빈도</p>

**문서 빈도(Document Frequency, DF)**란 한 단어가 얼마나 많은 문서에 나타나는지를 의미한다. 특정 단어가 많은 문서에 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산한다. 그러므로 3개의 문서에서 'movie'라는 단어가 4번 등장한다면 해당 단어의 DF 값은 3이 된다.
$$
DF(t,D)=count(t\in d:d\in D)
$$
DF 는 단어가 몇 개의 문서에서 등장하는지 계산한다. DF 값이 높으면 특정 단어가 많은 문서에서 등장한다고 볼 수 있다. 그 단어는 일반적으로 널리 사용되며, 중요도가 낮을 수 있다. 반대로 DF 값이 낮다면 특정 단어가 적은 수의 문서에만 등장한다는 뜻이다. 그러므로 특정한 문맥에서만 사용되는 단어일 가능성이 있으며, 중요도가 높을 수 있다.

<p style="font-size:125%">역문서 빈도</p>

**역문서 빈도(Inverse Document Frequency, IDF)**란 전체 문서 수를 문서 빈도로 나눈 다음에 로그를 취한 값을 말한다. 이는 문서 내에서 특정 단어가 얼마나 중요한지를 나타낸다. 문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미가 된다. 그러므로 문서 빈도의 역수를 취하면 단어의 빈도수가 적을수록 IDF 값이 커지게 보정하는 역할을 한다. 이를 통해 문서에서 특정 단어의 등장 횟수가 적으면 IDF는 상대적으로 커진다.
$$
IDF(t,D)=log({count(D)\over 1+DF(t,D)})
$$
IDF는 분모의 DF 값에 1을 더한 값을 사용한다. 특정 단어가 한 번도 등장하지 않는다면 분모가 0이 되는 경우가 발생한다. 그러므로 1과 같은 작은 값을 더해 분모가 0이 되는 결과를 방지한다. 추가로 IDF는 로그를 취한다. 전체 문서 수를 문서 빈도로 나눈 값을 사용한다면 너무 큰 값이 나올 수 있다. 10,000개의 문서에서 특정한 단어가 1번만 등장한다면 IDF 값은 5,000이 된다. 이러한 문제점을 방지하고자 로그를 취해 정교한 가중치를 얻는다.

<p style="font-size:125%">TF-IDF</p>

TF-IDF 는 앞선 단어 빈도와 역문서 빈도를 곲한 값으로 사용한다.
$$
TF-IDF(t,d,D)=TF(t,d)\times IDF(t,d)
$$
문서 내에 단어가 자주 등장하지만, 전체 문서 내에 해당 단어가 적게 등장한다면 TF-IDF 값은 커진다.그러므로 전체 문서에서 자주 등장할 확률이 높은 관사나 관용어 등의 가중치는 낮아진다.

사이킷 런(Scikit-learn) 라이브러리를 활용해 TF-IDF 를 계산하는 예지이다.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "This movie is famous movie",
    "I like this actor",
    "I don't like this actor"
]

tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer.fit(corpus)													# fit 메서드를 통해 말뭉치를 학습
tfidf_matrix = tfidf_vectorizer.transform(corpus)			# transform 메서드를 이용해 데이터 변환 수행

print(tfidf_matrix.toarray())													# 행은 하나의 문서, 열은 단어를 의미
print(tfidf_vectorizer.vocabulary_)										# 단어 사전 {키는 고유한 단어, 값은 색인 값}
```

**출력 결과**

```html
[[0.         0.         0.39687454 0.39687454 0.         0.79374908 0.2344005 ]
 [0.61980538 0.         0.         0.         0.61980538 0.         0.48133417]
 [0.4804584  0.63174505 0.         0.         0.4804584  0          0.37311881]]
{'this': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}
```

문서마다 중요한 단어만 추출할 수 있으며, 벡터값을 활용해 문서 내 핵심 단어를 추출할 수 있다. 하지만 출력 결과에서 확인할 수 있듯이 빈도 기반 벡터화는 문장의 순서나 문맥을 고려하지 않는다. 그러므로 문장 생성과 같이 순서가 중요한 작업에는 부적합하다. 또한 벡터가 해당 문서 내의 중요도를 의미할 뿐, 벡터가 단어의 의미를 담고 있지는 않다.

### Word2Vec

**Word2Vec**은 2013년 구글에서 공개한 임베딩 모델로 단어 간의 유사성을 측정하기 위해 **분포 가설(distributional hypothesis)**을 기반으로 개발되었다. 분포 가설이란 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이다. 분포 가설은 단어 간의 **동시 발생(co-occurrence)** 확률 분포를 이용해 단어 간의 유사성을 측정한다. 예를 들어 '내일 자동차를 타고 부산에 간다'와 '내일 비행기를 타고 부산에 간다'라는 두 문장에서 '자동차'와 '비행기'는 주변에 분포한 단어들이 동일하거나 유사하므로 비슷한 의미를 가질 것이라고 예상한다. 이러한 가정을 통해 단어의 **분산 표현(Distributed Representation)**을 학습할 수 있다. 분산 표현이란 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담는 것을 의미한다. 분포 가설에 따라 단어의 의미는 문맥상 분포적 특성을 통해 나타난다. 즉, 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 된다. 즉, '자동차'와 '비행기'라는 단어는 벡터 공간에서 서로 가까운 위치에 표현된다. 이러한 방법으로 빈도 기반의 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화해 단어의 의미 정보를 효과적으로 표현한다. 이러한 분산 표현 방식은 다양한 자연어 처리 작업에서 높은 성능을 보여주며, 다운스트림 작업에서 더 뛰어난 성능을 보인다.

#### 단어 벡터화

단어를 벡터화하는 방법은 크게 **희소 표현(sparse representation)**과 **밀집 표현(dense representation)**으로 나눌 수 있다. 원핫 인코딩, TF-IDF 등의 빈도 기반 방법은 희소 표현이며, Word2Vec 은 밀집 표현이다. 희소 표현 방법은 단어 사전의 크기가 커지면 벡터의 크기도 커지므로 공간적 낭비가 발생한다. 또한, 단어 간의 유사성을 반영하지 못하고, 벡터 간의 유사성을 계산하는 데도 많은 비용이 발생한다. 이에 비해 밀집 표현은 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않는다. 벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수. ㅣㅆ으며, 벡터의 대부분이 0이 아닌 실수로 이루어져 있어 효율적으로 공간을 활용할 수 있다. 밀집 표현 벡터화는 학습을 통해 단어를 벡터화하기 때문에 단어의 의미를 비교할 수 있다. 밀집 표현된 벡터를 **단어 임베딩 벡터(Word Embedding Vector)**라고 하며, Word2Vec 은 대표적인 단어 임베딩 기법 중 하나다. Word2Vec 은 밀집 표현을 위해 CBoW 와 Skip-gram 이라는 두 가지 방법을 사용한다.

#### CBoW

**CBoW(Continuous Bag of Words)**란 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법이다. **중심 단어(Centre Word)**는 예측해야 할 단어를 의미하며, 예측에 사용되는 단어들은 **주변 단어(Context Word)**라고 한다. 중심 단어를 맞추기 위해 몇 개의 주변 단어를 고려할지를 정해야 하는데, 이 범위를 **윈도(Window)**라고 한다. 이 윈도를 활용해 주어진 하나의 문장에서 첫 번째 단어부터 중심 단어로 하여 마지막 단어까지 학습한다. 학습을 위해 윈도를 이동해 가며 학습하는데, 이러한 방법을 **슬라이딩 윈도(Sliding Window)**라 한다. CBoW는 슬라이딩 윈도를 사용해 한 번의 학습으로 여러 개의 중심 단어와 그에 대한 주변 단어를 학습할 수 있다. 학습 데이터는 [주변 단어|중심 단어]로 구성된다. 이를 통해 대량의 말뭉치에서 효율적으로 단어의 분산 표현을 학습할 수 있다. CBoW 모델은 각 입력 단어의 원핫 벡터를 입력값으로 받는다. 입력 문장 내 모든 단어의 임베딩 벡터를 평균 내어 중심 단어의 임베딩 벡터를 예측한다. 입력 단어는 원핫 벡터로 표현돼 **투사층(Projection Layer)**에 입력된다. 투사층이란 원핫 벡터의 인뎃스에 해당하는 임베딩 벡터를 반환하는 **순람표(Lookup table, LUT)** 구조가 된다. 투사층을 통과하면 각 단어는 E 크기의 임베딩 벡터로 변환된다.

#### Skip-gram

**Skip-gram**은 CBoW 와 반대로 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다. 따라서 Skip-gram 은 중심 단어를 기준으로 양쪽으로 윈도 크기만큼의 단어들을 주변 단어로 삼아 훈련 데이터세트를 만든다. Skip-gram 과 CBoW 는 학습 데이터의 구성 방식에 차이가 있다. CBoW 는 하나의 윈도에서 하나의 학습 데이터가 만들어지는 반면, Skip-gram 은 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어진다. 하나의 중심 단어를 통해 여러 개의 주변 단어를 예측하므로 더 많은 학습 데이터세트를 추출할 수 있으며, 일반적으로 CBoW 보다 더 뛰어난 성능을 보인다. 또한 비교적 드물게 등장하는 단어를 더 잘 학습할 수 있게 되고 단어 벡터 공간에서 더 유의미한 거리 관계를 형성할 수 있다.

Skip-gram 모델도 CBoW 와 마찬가지로 입력 단어의 원핫 벡터를 투사층에 입력하여 해당 단어의 임베딩 벡터를 가져온다.

**기본 Skip-gram 클래스**

```python
from torch import nn

class VanillaSkipgram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim
        )
        self.linear = nn.Linear(
            in_features=embedding_dim,
            out_features=vocab_size
        )

    def forward(self, input_ids):
        embeddings = self.embedding(input_ids)
        output = self.linear(embeddings)
        return output
```

모델은 단순히 입력 단어와 주변 단어를 룩업 테이블에서 가져와서 내적을 계산한 다음, 손실 함수를 통해 예측 오차를 최소화하는 방식으로 학습된다. 데이터세트는 코포라 라이브러리의 네이버 영화 리뷰 감정 분석 데이터세트를 불러온다.

**영화 리뷰 데이터세트 전처리**

```python
import pandas as pd
from Korpora import Korpora
from konlpy.tag import Okt

corpus = Korpora.load("nsmc")
corpus = pd.DataFrame(corpus.test)

tokenizer = Okt()
tokens = [tokenizer.morphs(review) for review in corpus.text]
print(tokens[:3])
```

**출력 결과**

```python
[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]
```

**단어 사전 구축**

```python
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens):
    counter = Counter()
    for tokens in corpus:
        counter.update(tokens)
    vocab = special_tokens
    for token, count in counter.most_common(n_vocab):
        vocab.append(token)
    return vocab

vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=["<unk>"])
token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}

print(vocab[:10])
print(len(vocab))
```

**출력 결과**

```python
['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']
5001
```

Okt 토크나이저를 통해 토큰화된 데이터를 활용해 build_vocab 함수로 단어 사전을 구축한다. n_vocab 매개변수는 구축할 단어 사전의 크기를 의미한다. 만약 문서 내에 n_vocab 보다 많은 종류의 토큰이 있다면, 가장 많이 등장한 토큰 순서로 사전을 구축한다. special_tokens 는 특별한 의미를 갖는 토큰들을 의미한다. <unk> 토큰은 OOV에 대응하기 위한 토큰으로 단어 사전 내에 없는 모든 단어는 <unk> 토큰으로 대체된다.

**Skip-gram 의 단어 쌍 추출**

```python
def get_word_pairs(tokens, window_size):
    pairs = []
    for sentence in tokens:
        sentence_length = len(sentence)
        for idx, center_word in enumerate(sentence):
            window_start = max(0, idx - window_size)
            window_end = min(sentence_length, idx + window_size + 1)
            center_word = sentence[idx]
            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]
            for context_word in context_words:
                pairs.append([center_word, context_word])
    return pairs

word_pairs = get_word_pairs(tokens, window_size=2)
print(word_pairs[:5])
```

**출력 결과**

```python
[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]
```

get_word_pairs 함수는 토큰을 입력받아 Skip-gram 모델의 입력 데이터로 사용할 수 있게 전처리한다. window_size 는 주변 단어를 몇 개까지 고려할 것인지를 설정한다. 각 문장에서는 중심 단어와 주변 단어를 고려하여 쌍을 생성한다. idx 는 현재 단어의 인덱스를 나타내며, center_word 는 중심 단어를 나타낸다. 그리고 window_start 와 window_end 는 현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지를 결정한다. 두 매서드는 문장의 경계를 넘어가는 경우가 없게 조정한다.

**인덱스 쌍 변환**

```python
def get_index_pairs(word_pairs, token_to_id):
    pairs = []
    unk_index = token_to_id["<unk>"]
    for word_pair in word_pairs:
        center_word, context_word = word_pair
        center_index = token_to_id.get(center_word, unk_index)
        context_index = token_to_id.get(context_word, unk_index)
        pairs.append([center_index, context_index])
    return pairs

index_pairs = get_index_pairs(word_pairs, token_to_id)
print(index_pairs[:5])
```

**출력 결과**

```python
[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]
```

get_index_pairs 함수는 get_word_pairs 함수에서 생성된 단어 쌍을 토큰 인덱스 쌍으로 변환한다. 앞서 생성한 word_pairs 와 단어와 단어에 해당하는 ID 를 매핑한 딕셔너리인 token_to_id 로 인덱스 쌍을 생성한다. 딕셔너리의 get 매서드로 토큰이 단어 사전 내에 있으면 해당 토큰의 인덱스를 반환하고, 단어 사전 내에 없다면 <unk> 토큰의 인덱스를 반환한다. 이렇게 생성된 인덱스 쌍은 Skip-gram 모델의 입력 데이터로 사용된다.

**데이터로더 적용**

```python
import torch
from torch.utils.data import TensorDataset, DataLoader

index_pairs = torch.tensor(index_pairs)
center_index = index_pairs[:,0]
context_index = index_pairs[:,1]

dataset = TensorDataset(center_index, context_index)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

index_pairs 는 get_index_pairs 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루어진 리스트다. 이 리스트를 텐서 형식으로 변환한다. 이 텐서는 [N, 2]의 구조를 가지므로 중심 단어와 주변 단어로 나눠 데이터세트로 변환한다. 인덱스 쌍을 텐서 데이터세트로 변환하고 데이터로더에 적용했다면 모델을 학습하기 위해 필요한 작업을 진행한다.

**Skip-gram 모델 준비 작업**

```python
from torch import optim

device = "mps" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else "cpu"
word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.SGD(word2vec.parameters(), lr=0.1)
```

VanillaSkipgram 클래스의 단어 사전 크기(vocab_size)에 전체 단어 집합의 크기를 전달하고 임베딩 크기(embedding_dim)는 128로 할당한다. 손실함수는 단어 사전 크기만큼 클래스가 있는 분류 문제이므로 교차 엔트로피를 사용한다. 교차 엔트로피는 내부적으로 소프트맥스 연산을 수행하므로 신경망의 출력값을 후처리 없이 활용할 수 있다. 모델 준비 작업이 완료됐다면 모델 학습 코드를 구성한다.

**모델 학습**

```python
for epoch in range(10):
    cost = 0.0
    for input_ids, target_ids in dataloader:
        input_ids = input_ids.to(device)
        target_ids = target_ids.to(device)

        logits = word2vec(input_ids)
        loss = criterion(logits, target_ids)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cost += loss
    cost = cost / len(dataloader)
    print(f"Epoch : {epoch+1:4d}, Cost : {cost:.3f}")
```

**출력 결과**

```python
Epoch :    1, Cost : 6.196
Epoch :    2, Cost : 5.981
Epoch :    3, Cost : 5.932
Epoch :    4, Cost : 5.902
Epoch :    5, Cost : 5.880
Epoch :    6, Cost : 5.862
Epoch :    7, Cost : 5.847
Epoch :    8, Cost : 5.834
Epoch :    9, Cost : 5.823
Epoch :   10, Cost : 5.813
```

학습 결과는 위와 같았다. 간단하게 학습이 돌아가는 것만을 확인하였다.

**임베딩 값 추출**

```python
token_to_embedding = dict()
embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()

for word, embedding in zip(vocab, embedding_matrix):
    token_to_embedding[word] = embedding

index = 30
token = vocab[30]
token_embedding = token_to_embedding[token]
print(token)
print(token_embedding)
```

**출력 결과**

```python
연기
[-7.5715286e-01  4.0590358e-01 -1.1899692e+00 -9.6207671e-02
 -1.0968562e+00 -1.0633422e-01 -1.7805649e+00 -1.0999224e+00
 -9.8144656e-01 -8.6950880e-01 -1.2320224e+00  7.7908641e-01
...
  5.4985732e-02  7.0357382e-01 -7.6419038e-01 -1.7852409e+00
 -9.0001100e-01  5.7236212e-01 -1.3774593e-01  1.1110962e-01
 -1.3285615e-01  5.1559955e-01 -7.1339709e-01  4.0483725e-01]
```

Word2Vec 모델의 임베딩 행렬을 이용해 각 단어의 임베딩 값을 매핑하고, 인덱스 30 값의 단어와 임베딩 값을 출력한다. 이 임베딩 값으로 단어 간의 유사도를 확인할 수 있다. 임베딩의 유사도를 측정할 때는 **코사인 유사도(Cosine Similarity)**가 가장 일반적으로 사용되는 방법이다. 코사인 유사도는 두 벡터 간의 각도를 이용하여 유사도를 계산하며, 두 벡터가 유사할수록 값이 1에 가까워지고, 다를수록 0에 가까워진다. 두 벡터 간의 코사인 유사도는 두 벡터의 내적을 벡터의 크기의 곱으로 나누어 계산할 수 있다.
$$
cosine\ similarity(a,b)={a\cdot b\over ||a||\ ||b||}
$$
a 와 b는 유사도를 계산하려는 벡터이며, 두 벡터를 내적한 벡터의 크기의 곱을 나눠 코사인 유사도를 계산할 수 있다. 벡터의 크기는 각 성분의 제곱합에 루트를 씌운 값이다. 코사인 유사도는 임베딩 공간에서 단어 간의 유사도를 측정하는 데 매우 유용하다.

**단어 임베딩 유사도 계산**

```python
import numpy as np
from numpy.linalg import norm

def cosine_similarity(a, b):
    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))
    return cosine

def top_n_index(cosine_matrix, n):
    closet_indexes = cosine_matrix.argsort()[::-1]
    top_n = closet_indexes[1 : n+1]
    return top_n

cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)
top_n = top_n_index(cosine_matrix, n=5)

print(f"{token}와 가장 유사한 5개 단어")
for index in top_n:
    print(f"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}")
```

**출력 결과**

```python
연기와 가장 유사한 5개 단어
야함 - 유사도 : 0.2843
스릴러물 - 유사도 : 0.2823
2010년 - 유사도 : 0.2741
거장 - 유사도 : 0.2705
조화 - 유사도 : 0.2686
```

넘파이의 선형대수 라이브러리를 활용해 벡터와 벡터의 코사인 유사도나 벡터와 행렬의 코사인 유사도를 쉽계 계산할 수 있다. cosine_similarity 함수는 입력 단어와 단어 사전 내의 모든 단어와의 코사인 유사도를 계산한다. a 매개변수는 임베딩 토큰을 의미하며, b 매개변수는 임베딩 행렬을 의미한다. 임베딩 행렬은 [5001, 128]의 구조를 가지므로 노름을 계산할 때 axis=1 방향으로 계산한다. top_n_index 함수는 유사도 행렬을 내림차순으로 정렬해 어떤 단어가 가장 가까운 단어인지 반환한다. 입력 단어도 단어 사전에 포함되므로 입력 단어 자신이 가장 가까운 단어가 된다. 그러므로 두 번째 가까운 단어부터 계산해 반환한다. 입력된 단어가 '연기'일 때 가장 유사한 단어 3개로 '야함', '스릴러물', '2010년'이 추출된 것을 확인할 수 있다.

<p style="font-size:125%">계층적 소프트맥스</p>

**계층적 소프트맥스(Hierachical Softmax)**는 출력층을 **이진 트리(Binary tree)** 구조로 표현해 연산을 수행한다. 이때 자주 등장하는 단어일수록 트리의 상위 노드에 위치하고, 드물게 등장하는 단어일수록 하위 노드에 배치된다. 이러한 방식으로 확률을 계산하면 일반적인 소프트맥스 연산에 비해 빠른 속도와 효율성을 보인다. 각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터와 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산한다. **잎 노드(Leaf Node)**는 가장 깊은 노드로, 각 단어를 의미하며, 모델은 각 노드의 벡터를 최적화하여 단어를 잘 예측할 수 있게 한다. 각 단어의 확률은 경로 노드의 확률을 곱해서 구할 수 있다. 단어 사전의 크기를 $V$ 라고 했을 때 일반적인 소프트맥스 연산은 $O(V)$ 의 시간 복잡도를 갖지만, 계층적 소프트맥스의 시간 복잡도는 $O(log_2 V)$ 의 시간 복잡도를 갖는다. 

<p style="font-size:125%">네거티브 샘플링</p>

**네거티브 샘플링(Negative Sampling)**은 Word2Vec 모델에서 사용되는 확률적인 생플링 기법으로 전체 단어 집합에서 일부 단어를 샘플링하여 오답 단어로 사용한다. 학습 윈도 내에 등장하지 않는 단어를 n 개 추출하여 정답 단어와 함께 소프트맥스 연산을 수행한다. 이를 통햏 전체 단어의 확률을 계산할 필요 없이 모델을 효율적으로 학습할 수 있다.
$$
P(w_i)={f(w_i)^0.75\over \sum_{j=0}^V f(w_j)^0.75}
$$
네거티브 샘플링 추출 확률을 계산하기 위해 먼저 각 단어 $w_i$ 의 출현 빈도수를 $f(w_i)$ 로 나타낸다. 가령 말뭉치 내에 단어 '추천해요'가 100번 등장했고, 전체 단어의 빈도가 2,000이라면 $f(추천해요)={100\over 2000}$ 가 된다. $P(w_i)$ 는 단어 $w_i$ 가 네거티브 샘플로 추출될 확률이다. 이 때 출현 빈도수에 0.75 제곱한 값을 정규화 상수로 사용하는데, 이 값은 실험을 통해 얻어진 최적의 값이다.

네거티브 샘플링에서는 입력 단어 쌍이 데이터로부터 추출된 단어 쌍인지, 아니면 네거티브 샘플링으로 생성된 단어 쌍인지 이진 분류를 한다. 이를 위해 로지스틱 회귀 모델을 사용하며, 이 모델의 학습 과정에서는 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습한다. 네거티브 샘플링 Word2Vec 모델은 실제 데이터에서 추출된 단어 쌍은 1로, 네거티브 생플링을 통해 추출된 가짜 단어 쌍은 0으로 레이블링한다. 즉, 다중 분류에서 이진 분류로 학습 목접이 바뀌게 된다. 

#### Gensim

매우 간단한 구조의 Word2Vec 모델을 학습할 때 데이터 수가 적은 경우에도 학습하는 데 오랜 시간이 소요된다. 이러한 경우, 계층적 소프트맥스나 네거티브 생플링 같은 기법을 사용하면 더 효율적으로 학습할 수 있다. **젠심(Gensim)** 라이브러리를 활용하면 Word2Vec 과 같은 자연어 처리 모델을 쉽게 구성할 수 있다. 젠심 라이브러리는 대용량 텍스트 데이터의 처리를 위한 메모리 효율적인 방법을 제공해 대규모 데이터 세트에서도 효과적으로 모델을 학습할 수 있다. 또한 학습된 모델을 저장하여 관리할 수 있고, 비슷한 단어 찾기 등 유사도와 관련된 기능도 제공하여 자연어 처리에 필요한 다양한 기능을 제공한다. 젠심 라이브러리는 사이썬(Cython)을 이용해 병렬 처리나 네거티브 샘플링 등을 적용한다. 사이썬은 C++ 기반의 확장 모듈을 파이썬 모듈로 컴파일하는 기능을 제공한다. 젠심으로 모델을 구성한다면 파이토치를 이용한 학습보다 훨씬 더 빠른 속도로 학습할 수 있다. 



### 순환신경망

**순환 신경망(Recurrent Neural Network, RNN)** 모델은 순서가 있는 연속적인 데이터를 처리하는 데 적합한 구조를 갖고 있다. 순환 신경망은 각 **시점(Time step)**의 데이터가 이전 시점의 데이터와 독립적이지 않다는 특성 때문에 효과적으로 작동한다. 특정 시점 t에서의 데이터가 이전 시점의 영향을 받는 데이터를 연속형 데이터라 한다. 자연어 데이터는 연속적인 데이터의 일종으로 볼 수 있다. 자연어 데이터는 문장 안에서 단어들이 순서대로 등장하므로, 각 단어는 이전에 등장한 단어의 영향을 받아 해당 문장의 의미를 형성한다. 자연어는 한 단어가 이전의 단어들과 상호작용하여 문맥을 이루고 의미를 형성한다. 이러한 특징으로 인해 자연어는 연속형 데이터의 특성을 갖는다. 즉, t번째 단어는 t-1 번째까지의 단어에 영향을 받아 결정된다. 또한 긴 문장일수록 앞선 단어들과 뒤따르는 단어들 사이에 강한 **상관관계(Correlation)**가 존재한다.

#### 순환신경망

순환 신경망은 연속적인 데이터를 처리하기 위해 개발된 인공 신경망의 한 종류다. 이전에 처리한 데이터를 다음 단계에 활용하고 현재 입력 데이터와 함께 모델 내부에서 과거의 상태를 기억해 현재 상태를 예측하는 데 사용된다. 순환 신경망은 시계열 데이터, 자연어 처리, 음성 인식 및 기타 시퀀스 데이터와 같은 도메인에서 널리 사용된다. 이러한 데이터는 일반적으로 길이가 가변적이며, 순서에 따라 의미가 있기 때문에, 순환 신경망은 이러한 데이터를 처리하기에 적합한 구조를 가지고 있다. 순환 신견망은 연속형 데이터를 순서대로 입력받아 처리하며 각 시점마다 **은닉 상태(Hidden state)**의 형태로 저장한다. 각 시점의 데이터를 입력으로 받아 은닉 상태와 출력값을 계산하는 노드를 순환 신경망의 **셀(Cell)**이라 한다. 순환 신경망의 셀은 이전 시점의 은닉 상태 $h_{t-1}$을 입력으로 받아 현재 시점의 은닉 상태 $h_t$를 계산한다. 이러한 재귀적 특징으로 '순환' 신견망이라 불린다.

순환 신경망은 각 시점 t에서 현재 입력값 $x_t$ 와 이전 시점 $t-1$ 의 은닉 상태 $h_{t-1}$ 를 이용해 현재 시점의 은닉 상태 $h_t$ 와 출력값 $y_t$ 를 계산한다. 먼저 은닉 상태의 수식은 다음과 같다
$$
h_t=\sigma(h_{t-1},x_t),\
h_t=\sigma(W_{hh}h_{t-1}+W_{xh}x_t+b_h)
$$
$\sigma_h$ 는 순환 신경망의 은닉 상태를 계산하기 위한 활성화 함수를 의미한다. 은닉 상태 활성화 함수는 이전 시점 $t-1$ 의 은닉 상태 $h_{t-1}$ 과 입력값 $x_t$ 를 입력받아 현재 시점의 은닉 상태 $h_t$ 를 계산한다. 이때, $\sigma_h$ 는 가중치($W$)와 편향($b$)를 이용해 계산한다. $W_{hh}$는 이전 시점의 은닉 상태 $h_{t-1}$에 대한 가중치, $W_{xh}$ 는 입력값 $x_t$에 대한 가중치, $b_h$는 은닉 상태 $h_t$의 편향을 의미한다. 다음은 출력값 계산 방법이다.
$$
y_t=\sigma_y(h_t),\ y_t=\sigma_y(W_{hy}h_t+b_y)
$$
$\sigma_y$는 순환 신견망의 출력값을 계산하기 위한 활성화 함수를 의미한다. 출력값 활성화 함수는 현재 시점의 은닉 상태 $h_t$를 입력으로 받아 출력값 $y_t$를 계산한다. 출력값 계산 방법도 가중치($W$)와 편향($b$)를 이용해 계산한다. $W_{hy}$는 현재 시점의 은닉 상태 $h_t$에 대한 가중치, $b_y$는 출력값 $y_t$의 편향을 의미한다. 순환 신경망의 출력값은 이전 시점의 정보를 현재 시점에서 활용해 입력 시퀀스의 패턴을 파악하고 출력 값을 예측하므로 연속형 데이터를 처리할 수 있다. 순환 신경망은 다양한 구조로 모델을 설계할 수 있다. 가장 단순한 구조인 단순 순환 구조부터 일대다 구조, 다대일 구조, 다대다 구조 등이 있다.

#### 일대다 구조

**일대다 구조(One-to-Many)**는 하나의 입력 시퀀스에 대해 여러 개의 출력값을 생성하는 순환 신경망 구조다. 예를 들어, 자연어 처리 분야에서는 일대다 구조를 사용하여 문장을 입력으로 받고, 문장에서 각 단어의 품사를 예측하는 작업을 할 수 있다. 입력 시퀀스는 문장으로 이루어져 있으며, 출력 시퀀스는 각 단어의 품사로 이루어져 있다. 이미지 데이터를 입력으로 받으면 이미지에 대한 설명을 출력하는 **이미지 캡셔닝(Image Captioning) **모델이 된다. 입력 시퀀스는 이미지로 이루어져 있으며, 출력값은 이미지에 대한 설명 문장들로 구성된다. 이러한 일대다 구조를 구현하기 위해서는 출력 시퀀스의 길이를 미리 알고 있어야 한다. 이를 위해 입력 시퀀스를 처리하면서 시퀀스의 정보를 활용해 출력 시퀀스의 길이를 예측하는 모델을 함께 구현해야 한다.

#### 다대일 구조

**다대일 구조(Many-to-One)**는 여러 개의 입력 시퀀스에 대해 하나의 출력값을 생성하는 순환 신경망 구조다. 예를 들어 **감성 분류(Sentiment Analysis)** 분야에서는 다대일 구조를 사용하여 특정 문장의 감정(긍정, 부정)을 예측하는 작업을 할 수 있다. 이때, 입력 시퀀스는 문장으로 이루어져 있으며, 출력값은 해당 문장의 감정(긍정, 부정)으로 이루어져 있다. 입력 시퀀스가 어떤 범주에 속하는지를 구분하는 문장 분류, 두 문장 간의 관계를 추론하는 **자연어 추론(Nature Language Inference)** 등에도 적용할 수 있다.

#### 다대다 구조

**다대다 구조(Many-to-Many)**는 입력 시퀀스와 출력 시퀀스의 길이가 여러 개인 경우에 사용되는수노한 신경망 구조다. 이 구조는 다양한 분야에서 활용되며, 예를 들어 입력 문장에 대해 번역된 출력 문장을 생성하는 번역기, 음성 인식 시스템에서 음성 신호를 입력으로 받아 문장을 출력하는 음성 인식기 등에서 사용된다. 다대다 구조에서도 입력 시퀀스와 출력 시퀀스의 길이가 서로 다른 경우가 있을 수 있다. 예를 들어, 입력 문장의 길이와 출력 문장의 길이가 일치하지 않는 경우다. 이 경우에는 입력 시퀀스와 출력 시퀀스의 길이를 맞추기 위해 패딩을 추가하거나 잘라내는 등의 전처리 과정이 수행된다. 다대다 구조는 **시퀀스-시퀀스(Seq2Seq)** 구조로 이뤄져 있다. 시퀀스-시퀀스 구조는 입력 시퀀스를 처리하는 **인코더(Encoder)**와 출력 시퀀스를 생성하는 **디코더(Decoder)**로 구성된다. 인코더는 입력 시퀀스를 처리해 고정 크기의 벡터를 출력하고, 디코더는 이 벡터를 입력으로 받아 출력 시퀀스를 생성한다. 

#### 양방향 순환 신경망

**양방향 순환 신경망(Bidirectional Recurrent Neural Network, BiRNN)**은 기본적인 순환 신경망에서 시간 방향을 양방향을 처리할 수 있도록 고안된 방식이다. 순환 신경망은 현재 시점의 입력값을 처리하기 위해 시점($t-1$)의 은닉 상태를 이용하는데, 양방향 순환 신경망에서는 이전 시점($t-1$)의 은닉 상태뿐만 아니라 이후 시점($t+1$)의 은닉 상태도 함께 이용한다. 양방향 순환 신경망은 $t$ 시점 이후의 데이터도 $t$ 시점의 데이터를 예측하는 데 사요오딜 수 있다. 이러한 방법은 입력 데이터를 순방향으로 처리하는 것만 아니라 역방향으로 거꾸로 읽어 들여 처리하는 방식으로 이루어진다. 대부분 연속형 데이터는 이전 시점 데이터뿐만 아니라 이후 시점의 데이터와 큰 상관관계를 갖고 있다. 그러므로 양방향 순환 신경망은 양방향적인 정보를 모두 고려하여 현재 시점의 출력값을 계산한다.

#### 다중 순환 신경망

**다중 순환 신경망(Stacked Recurrent Neural Network)**은 여러 개의 순환 신경망을 연결하여 구성한 모델로 각 순환 신경망이 서로 다른 정보를 처리하도록 설계돼 있다. 다중 순환 신경망은 여러 개의 순환 신경망 층으로 구서오디며, 각 층에서의 출력값은 다음 층으로 전달되어 처리된다. 이렇게 여러 개의 층으로 구성된 RNN은 데이터의 다양한 특징을 추출할 수 있어 성능이 향상될 수 있다. 또한, 층이 깊어질수록 더 복잡한 패턴을 학습할 수 있다는 장점이 있다. 하지만 순환 신경망 층이 많아질수록 학습 시간이 오래 걸리고, 기울기 소실 문제가 발생할 가능성도 높아진다.

```python
import torch
from torch import nn

input_size = 128
output_size =256
num_layers = 3
bidirectional = True

model = nn.RNN(
    input_size=input_size,
    hidden_size=output_size,
    num_layers=num_layers,
    nonlinearity="tanh",
    batch_first=True,
    bidirectional=bidirectional,
)

batch_size = 4
sequence_len = 6

inputs = torch.randn(batch_size, sequence_len, input_size)
h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)

outputs, hidden = model(inputs, h_0)
print(outputs.shape)
print(hidden.shape)
```

**출력 결과**

```python
torch.Size([4, 6, 512])
torch.Size([6, 4, 256])
```



#### 장단기 메모리

**장단기 메모리(Long Short Term Memory, LSTM)**는 기존 수노한 신경망이 갖고 있던 기억력 부족과 기울기 소실 문제를 해결한 모델이다. 일반적인 순환 신경망은 특정 시점에서 이전 입력 데이터의 정보를 이용해 출력값을 예측하는 구조이므로 시간적으로 먼 과거의 정보는 잘 기억하지 못한다. 하지만 어떤 연속형 데이터는 다음 시점의 데이터를 유추하기 위해 훨씬 먼 시점의 데이터에서 힌트를 얻어야 한다. 순환 신경망의 경우 시간적으로 연속된 데이터를 다룰 수 있다는 장점이 있지만, 앞선 시점에서의 정보를 끊임없이 반영하기에는 학습 데이터의 크기가 커질수록 앞서 학습한 정보가 충분히 전달되지 않는 다는 단점이 있다. 이러한 단점으로 인해 **장기 의존성 문제(Long-term dependencies)**가 발생할 수 있다. 또한, 활성화 함수로 사용되는 하이퍼볼릭 탄젠트 함수나 ReLU 함수의 특성으로 인해 역전파 과정에서 기울기 소실이나 기울기 폭주가 발생할 가능성이 있다. 이러한 문제를 해결하기 위해 장단기 메모리를 사용한다. 장단기 메모리는 순환 신경망과 비슷한 구조를 가지지만, **메모리 셀(Memory Cell)**과 **게이트(Gate)**라는 구조를 도입해 장기 의존성 문제와 기울기 소실 문제를 해결한다.

장단기 메모리는 셀 상태(Cell state)와 망각 게이트(Forget gate), 기억 게이트(Input gate), 출력 게이트(Output gate)로 정보의 흐름을 제어한다. 셀 상태는 정보를 저장하고 유지하는 메모리 역할을 하며 출력 게이트와 망각 게이트에 의해 제어된다. 망각 게이트는 장단기 메모리에서 이전 셀 상태에서 어떠한 정보를 삭제할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 삭제할지 결정한다. 입력 게이트는 새로운 정보를 어떤 부분에 추가할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 추가할지 결정한다. 출력 게이트는 셀 상태의 정보 중 어떤 부분을 출력할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태, 그리고 새로 추가된 정보를 입력으로 받아 출력할 정보를 결정한다. 





----

보고서 적을 때 부족한 부분 추가
